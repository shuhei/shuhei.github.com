<!doctype html>
  <html lang="en">
    <head>
      <meta charset="utf-8">
			<link rel="preload" href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital@0;1&family=Libre+Franklin:wght@700&family=DM+Mono&display=swap" as="style">
<link rel="preload" href="https://fonts.gstatic.com/s/librebaskerville/v9/kmKnZrc3Hgbbcjq75U4uslyuy4kn0qNZaxMaC82U.woff2" as="font" crossorigin="anonymous">
<link rel="preload" href="https://fonts.gstatic.com/s/librefranklin/v6/jizOREVItHgc8qDIbSTKq4XkRg8T88bjFuXOnduhycKkANDPTedX18mE.woff" as="font" crossorigin="anonymous">
<link rel="preload" href="https://fonts.gstatic.com/s/dmmono/v3/aFTU7PB1QTsUX8KYthqQBK6PYK0.woff2" as="font" crossorigin="anonymous">
      <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <title>Node.js under a microscope: CPU FlameGraph and FlameScope - Shuhei Kagawa</title>
      <link rel="icons" sizes="16x16 32x32 48x48" href="/favicon.ico">
      <link rel="alternate" type="application/rss+xml" title="RSS Feed for shuheikagawa.com" href="/blog/feed/rss.xml">
      <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-309586-8', 'shuheikagawa.com');
  ga('send', 'pageview');
</script>
      <style>:root{--bg-color:#fff;--text-color:#000;--highlight-color:#095ae8;--code-text-color:var(--code-mono-1);--syntax-hue:230;--syntax-saturation:1%;--syntax-brightness:100%;--code-mono-1:hsl(var(--syntax-hue), 8%, 24%);--code-mono-2:hsl(var(--syntax-hue), 6%, 44%);--code-mono-3:hsl(var(--syntax-hue), 4%, 64%);--code-hue-1:hsl(198, 99%, 37%);--code-hue-2:hsl(221, 76%, 47%);--code-hue-3:hsl(301, 63%, 40%);--code-hue-4:hsl(119, 72%, 31%);--code-hue-5:hsl(5, 68%, 48%);--code-hue-5-2:hsl(344, 84%, 43%);--code-hue-6:hsl(41, 99%, 30%);--code-hue-6-2:hsl(41, 99%, 30%);--body-font-family:"Libre Baskerville",serif;--heading-font-family:"Libre Franklin",sans-serif;--code-font-family:"DM Mono",monospace}body{font-family:"Libre Baskerville",serif;font-family:var(--body-font-family);background:#fff;background:var(--bg-color);color:#000;color:var(--text-color);padding:0;margin:0;line-height:1.7}.container{width:700px;padding:0 20px;margin:0 auto}a{color:#095ae8;color:var(--highlight-color);transition:color .5s ease}a:hover{opacity:.7}.header{padding:1.5em 0 1em;display:flex}.header__title{margin:0 10px 0 0;flex-grow:1;font-size:1.2em;font-weight:400}.header__title a{color:#000;color:var(--text-color);text-decoration:none}.header__nav{flex-grow:0}.menu{list-style-position:outside;list-style-type:none;padding:0;margin:0;text-align:right}.menu__item{display:inline-block;padding:.3em 0 0 .7em}.menu__item a{color:#000;color:var(--text-color);text-decoration:none}.footer{padding:50px 0 30px;text-align:center}.title{font-family:"Libre Franklin",sans-serif;font-family:var(--heading-font-family);font-size:3em;margin:0 0 .4em;line-height:1.1}.title a{color:#000;color:var(--text-color);text-decoration:none}.post,.post-list{padding:30px 0 20px}.post:not(:first-child):before{content:"* * *";font-family:"Libre Franklin",sans-serif;font-family:var(--heading-font-family);font-weight:700;font-size:4em;display:block;margin:.2em auto .6em;text-align:center;line-height:1}.post .meta{font-size:small;margin:10px 0 20px}.post-list .title{margin-bottom:10px}.post-list-item{line-height:1.6em;padding:10px 0;display:flex}.post-list-item__date{font-size:small;width:8em;flex-shrink:0}.post-list-item__title{font-size:large;margin:0;font-family:"Libre Franklin",sans-serif;font-family:var(--heading-font-family);font-weight:700}.post-list-item__title a{text-decoration:none;color:#000;color:var(--text-color)}.content h2,.content h3,.content h4,.content h5,.content h6{font-family:"Libre Franklin",sans-serif;font-family:var(--heading-font-family);line-height:1.1;margin:1em 0 0 0}.content h2{font-size:2em}.content h3{font-size:1.5em}.content img{max-width:100%}.responsive-image-wrapper{display:block}.responsive-image-outer{display:block;margin:0 auto}.responsive-image-inner{display:block;position:relative}.responsive-image{position:absolute;top:0;left:0}.social-buttons{margin:2em 0 0}.comments{margin-bottom:3em}ol,ul{list-style-position:outside;padding-left:1.4em}.table-wrapper{overflow-x:auto}table{border-collapse:collapse;margin:2em 0}th{font-weight:400;text-align:left}tbody{border-top:1px solid #333;padding:.5em 0}td,th{padding-right:1em}code{color:#383942;color:var(--code-text-color);font-size:.9em;font-family:"DM Mono",monospace;font-family:var(--code-font-family)}.code__filename{display:inline-block;margin-bottom:13px;padding:5px 10px;font-size:.85em;background-color:#666}.hljs{display:block;line-height:1.5em;padding:15px 20px 0;overflow-x:auto;-webkit-overflow-scrolling:touch;color:#383942;color:var(--code-text-color)}.hljs-comment,.hljs-quote{color:#696b76;color:var(--code-mono-2);font-style:italic}.hljs-doctag,.hljs-formula,.hljs-keyword{color:#a625a4;color:var(--code-hue-3)}.hljs-deletion,.hljs-name,.hljs-section,.hljs-selector-tag,.hljs-subst{color:#cd3527;color:var(--code-hue-5)}.hljs-literal{color:#0083bb;color:var(--code-hue-1)}.hljs-addition,.hljs-attribute,.hljs-meta-string,.hljs-regexp,.hljs-string{color:#188716;color:var(--code-hue-4)}.hljs-built_in,.hljs-class .hljs-title{color:#986800;color:var(--code-hue-6-2)}.hljs-attr,.hljs-number,.hljs-selector-attr,.hljs-selector-class,.hljs-selector-pseudo,.hljs-template-variable,.hljs-type,.hljs-variable{color:#986800;color:var(--code-hue-6)}.hljs-bullet,.hljs-link,.hljs-meta,.hljs-selector-id,.hljs-symbol,.hljs-title{color:#1c56d2;color:var(--code-hue-2)}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}.hljs-link{text-decoration:underline}blockquote{border-left:5px solid #000;border-left:5px solid var(--text-color);font-style:italic;padding:.1em 1.2em;margin-left:0;quotes:"\201C""\201D""\2018""\2019"}blockquote p:first-child:before{content:open-quote;font-family:serif;font-size:3em;font-weight:700;line-height:.1em;margin-right:.2em;vertical-align:-.4em}blockquote cite{color:#999990}blockquote cite:before{content:"- "}.pagination{list-style-position:outside;list-style-type:none;padding:0;margin-top:20px;display:flex;justify-content:space-between}.pagination li{min-height:1em;width:33%}.pagination a{text-decoration:none}.pagination__prev-page a:before{content:"< "}.pagination__next-page{text-align:right}.pagination__next-page a:after{content:" >"}.pagination__archives{flex-grow:1;text-align:center}@media only screen and (max-width:767px){.container{width:auto}.post,.post-list{padding-top:10px}.post-list-item{display:block}.hljs{margin-left:-20px;margin-right:-20px;padding:1.4em 20px;border-radius:0}li .hljs{margin-left:0;border-radius:.4em}.responsive-image-wrapper{margin-left:-20px;margin-right:-20px}}</style>
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital@0;1&family=Libre+Franklin:wght@700&family=DM+Mono&display=swap">
      <meta name="description" content="Last week, I had an opportunity to talk about profiling Node.js applications on production at an internal guild meeting at work. Here is a written version of it...">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@shuheikagawa">
<meta property="og:title" content="Node.js under a microscope: CPU FlameGraph and FlameScope - Shuhei Kagawa">
<meta property="og:site_name" content="Shuhei Kagawa">
<meta property="og:description" content="Last week, I had an opportunity to talk about profiling Node.js applications on production at an internal guild meeting at work. Here is a written version of it...">
<meta property="og:image" content="https://shuheikagawa.com/images/flamegraph.png">
    </head>
    <body>
      <div class="container">
        <header class="header">
          <h1 class="header__title">
            <a href="/">Shuhei Kagawa</a>
          </h1>
          <nav class="header__nav">
            <ul class="menu">
              <li class="menu__item">
                <a href="/about/">About</a>
              </li>
              <li class="menu__item">
                <a href="/blog/archives/">All posts</a>
              </li>
            </ul>
          </nav>
        </header>
        <div class="main">
          
    
    <div class="post">
      <div class="post-header">
        <h1 class="title">
          <a href=/blog/2018/09/16/node-js-under-a-microscope/>Node.js under a microscope: CPU FlameGraph and FlameScope</a>
        </h1>
        <div class="meta">
          <span class="date">Sep 16, 2018</span>
          - <span class="category">Node.js</span>, <span class="category">Linux</span>
        </div>
      </div>
      <div class="content">
        <div>
          <p>Last week, I had an opportunity to talk about profiling Node.js applications on production at an internal guild meeting at work. Here is a written version of it plus some additional information.</p>
<h2 id="background">Background</h2>
<p>I have been working on Node.js microservices, which fetch data from API servers and render HTML with React, at work. We monitor response times at load balancers, in application metrics and with distributed tracing with OpenTracing. One of the microservices had a weird gap between 99 percentile response times of itself and its dependencies. It was spending an extra 500 milliseconds—but I didn&#39;t know why.</p>
<p>My first suspect was the network. It is the place full of uncertainty. After learning and trying different commands and metrics, I took <code>tcpdump</code> and checked packets one by one with my eyes and a script. There were no significant delays that I had expected. So I had to stop blaming the network—or <em>someone else</em>.</p>
<h2 id="cpu-profiling-with-the-linux-perf-command">CPU profiling with the Linux <code>perf</code> command</h2>
<p>Because the weird latency was happening in the application itself, I wanted to know what&#39;s going on in it. There are mainly two ways to achieve this: profiling and tracing. Profiling records some samples and tracing records everything. I wanted to do it <strong>on production</strong>, so profiling was naturally a good fit because of its smaller overhead.</p>
<p>For Node.js, there are mainly two different tools. One is <a href="https://github.com/v8/v8/wiki/V8-Profiler">V8 profiler</a>, and the other is <a href="https://perf.wiki.kernel.org/index.php/Main_Page">Linux perf</a>. V8 profiler uses the profiler provided by V8. It covers all JavaScript executions and V8 native functions. It works on non-Linux operating systems. If you use non-Linux machines, it might be pretty handy. On the other hand, Linux <code>perf</code> can profile almost anything including Linux kernel, libuv, and all processes on your OS with minimal overhead. However, as the name suggests, it works only on Linux. According to <a href="https://github.com/nodejs/diagnostics/issues/148">Node CPU Profiling Roadmap</a>, it seems that V8 profiler is the one officially supported by the V8 team, but Linux <code>perf</code> will keep working for a while. After all, I picked Linux <code>perf</code> because of low performance-overhead and small intervention to applications.</p>
<p>Linux <code>perf record</code> records stack traces into a binary file called <code>perf.data</code> by default. The binary file has only addresses and file names of functions. <code>perf script</code> converts the stack traces into a human-readable text file adding function names from program binaries and symbol map files.</p>
<pre><code class="hljs sh"><span class="hljs-comment"># Install dependencies for `perf` command</span>
sudo apt-get install linux-tools-common linux-tools-$(uname -r)
<span class="hljs-comment"># Test `perf` command</span>
sudo perf top

<span class="hljs-comment"># Record stack traces 99 times per second for 30 seconds</span>
sudo perf record -F 99 -p <span class="hljs-variable">${pid}</span> -g -- sleep 30s
<span class="hljs-comment"># Generate human readable stack traces</span>
sudo perf script &gt; stacks.<span class="hljs-variable">${pid}</span>.out</code></pre><p>Now we have human-readable stack traces, but it&#39;s still hard to browse thousands of stack traces and get insights from them. How can we efficiently analyze them?</p>
<h2 id="cpu-flame-graph">CPU Flame Graph</h2>
<p><a href="http://www.brendangregg.com/flamegraphs.html">CPU Flame Graph by Brendan Gregg</a> is a great way of visualizing stack traces. It aggregates stack traces into one chart. Frequently executed functions are shown wider and rarely executed functions are narrower in the chart.</p>
<p><span class="responsive-image-wrapper"><span class="responsive-image-outer" style="max-width: 700px;"><span class="responsive-image-inner" style="padding-top: 55.714285714285715%;"><img class="responsive-image" src="/images/flamegraph.png" alt="CPU Flame Graph"></span></span></span>
<em>A CPU Flame Graph from <a href="https://github.com/shuhei/perf-playground">a sample application</a></em></p>
<p>I found some insights about the application on production with CPU Flame Graph:</p>
<ul>
<li>React server-side rendering is considered to be a very CPU-intensive task that blocks Node.js event loop. However, <code>JSON.parse()</code> was using 3x more CPU than React—it might be because we had already optimized React server-side rendering though.</li>
<li>Gzip decompression was using the almost same amount of CPU as React server-side rendering.</li>
</ul>
<p>There are a few tools like <a href="https://github.com/brendangregg/FlameGraph">FlameGraph</a> and <a href="https://github.com/davidmarkclements/0x">0x</a> to generate CPU Flame Graph from Linux <code>perf</code> stack traces. However, I eventually didn&#39;t need them because FlameScope, which I&#39;ll explain next, can generate CPU Flame Graph too.</p>
<h2 id="flamescope">FlameScope</h2>
<p><a href="https://github.com/Netflix/flamescope">FlameScope by Netflix</a> is another great tool for visualizing stack traces in a time-series. It shows a heatmap out of stack traces. Each cell represents a short amount of time, 20 ms if 50 cells per second, and its color represents how many times the process was on-CPU. It visualizes patterns of your application&#39;s activity.</p>
<p><span class="responsive-image-wrapper"><span class="responsive-image-outer" style="max-width: 700px;"><span class="responsive-image-inner" style="padding-top: 68.71428571428572%;"><img class="responsive-image" src="/images/flamescope-annotated.png" alt="FlameScope"></span></span></span>
<em>Image from <a href="https://github.com/Netflix/flamescope">Netflix/flamescope</a></em></p>
<p>If you select a time range on the heatmap, FlameScope shows you a CPU Flame Graph of the range. It allows you to examine what happened when in details.</p>
<p>To use FlameScope, check out the repository and run the python server. Then put stack trace files from <code>perf script</code> into <code>examples</code> directory, and open <code>http://localhost:5000</code>.</p>
<p>I found a couple of exciting insights about the application on production using this tool.</p>
<h3 id="example-1-heavy-tasks-in-the-master-process">Example 1: Heavy tasks in the master process</h3>
<p>The application used <a href="https://nodejs.org/api/cluster.html">the <code>cluster</code> module</a> to utilize multiple CPU cores. FlameScope showed that the master process was not busy for most of the time, but it occasionally kept using CPU for 1.5 seconds continuously! FlameScope showed that it was caused by metrics aggregation.</p>
<p>The master process was aggregating application metrics from worker processes, and it was responding to metrics collectors a few times in a minute. When the metrics collectors asked for data, the master process calculated percentiles of response times and prepared a JSON response. The percentile calculation was taking long time because the application had a lot of metrics buckets and the library that we used was using <code>JSON.stringify()</code> and <code>JSON.parse()</code> to deep-copy objects!</p>
<h3 id="example-2-frequent-garbage-collections">Example 2: Frequent garbage collections</h3>
<p>FlameScope showed that the worker processes were not overloaded for most of the time, but they had a few hundred milliseconds of CPU-busy time in about 10 seconds. It was caused by mark-sweep and mark-compact garbage collections.</p>
<p>The application had an in-memory fallback cache for API calls that was used only when API calls and retries fail. Even when API had problems, the cache hit rate was very low because of the number of permutations. In other words, it was not used almost at all. It cached large API responses for a while and threw them away after the cache expired. It looked innocent at first glance—but it was a problem for V8&#39;s <a href="http://www.memorymanagement.org/glossary/g.html#term-generational-garbage-collection">generational garbage collector</a>.</p>
<p>The API responses were always promoted to the old generation space causing frequent slow GCs. GC of the old generation is much slower than GC of the young generation. After removing the fallback cache, the application&#39;s 99 percentile response time improved by hundreds of milliseconds!</p>
<h2 id="nodejs-gotchas">Node.js gotchas</h2>
<p><code>perf script</code> collects symbols for function addresses from program binaries. For Node.js, we need something special because functions are compiled just in time. As far as I know, there are two ways to record symbols:</p>
<ol>
<li>Run your Node.js process with <code>--perf-basic-prof-only-functions</code> option. It generates a log file at <code>/tmp/perf-${pid}.map</code>. The file keeps growing. The speed depends on your application, but it was a few megabytes per day for an application at work. Another problem is that functions in V8 keep moving and the addresses in <code>/tmp/perf-${pid}.map</code> get outdated. <a href="https://gist.github.com/shuhei/6c261342063bad387c70af384c6d8d5c">I wrote a script to fix the issue</a>.</li>
<li>Use <a href="https://github.com/mmarchini/node-linux-perf">mmarchini/node-linux-perf</a>. It generates the same <code>/tmp/perf-${pid}.map</code> as <code>--perf-basic-prof-only-functions</code> does, but on demand. Because it always freshly generates the file, it doesn&#39;t contain outdated symbols. It seems to be the way to go, but I haven&#39;t tried this on production yet.</li>
</ol>
<p>In addition to the above, there are a few more Node.js options that you can use to improve your stack traces—though I haven&#39;t tried them on production because the stack traces were already good enough for me:</p>
<ul>
<li><code>--no-turbo-inlining</code> turns off function inlining, which is an optimization done by V8. Because function inlining fuses multiple functions into one, it can make it harder to understand stack traces. Turning it off generates more named frames.</li>
<li><code>--interpreted-frames-native-stack</code> fixes <code>Builtin:InterpereterEntryTrampoline</code> in stack traces. It is available from Node.js 10.4.0. Check out &quot;Interpreted Frames&quot; in <a href="https://github.com/nodejs/diagnostics/issues/148#issuecomment-369348961">Updates from the Diagnostics Summit</a> for more details.</li>
</ul>
<h2 id="docker-gotchas">Docker gotchas</h2>
<p>It gets a bit tricky when you are using containers to run your application. There are two ways to use Linux <code>perf</code> with Docker:</p>
<ol>
<li>Run <code>perf record</code> and <code>perf script</code> in the same Docker container as your application is running</li>
<li>Run <code>perf record</code> and <code>perf script</code> in the host OS</li>
</ol>
<p>I eventually chose the option 2. I tried the option 1 first but gave up because I was using Alpine Linux as the base image and it was hard to make Linux <code>perf</code> available on it.</p>
<p>To run <code>perf record</code> in the host OS, we need to figure out <code>pid</code> of the application process in the host.</p>
<pre><code class="hljs ">$ ps ax | grep -n &#39;node --perf&#39;
21574 pts/0    Sl+    2:53 node --perf-basic-prof-only-functions src/index.js
30481 pts/3    S+     0:00 grep --color=auto node --perf
# or
$ pgrep -f &#39;node --perf&#39;
21574

$ sudo perf record -F 99 -p 21574 -g -- sleep 30s</code></pre><p><code>perf script</code> collects symbols from binaries and symbol files to get human-readable function names. It needs to be able to read the binaries whose functions were recorded with <code>perf script</code> and <code>/tmp/${pid}.map</code> files that applications generate. However, <code>perf script</code> in the host OS cannot read them with the same file names as the container can. (It seems that this is not the case anymore with the latest Linux kernel because <a href="https://lkml.org/lkml/2017/7/19/790">its <code>perf</code> command knows containers</a>. But it was the case for me because I was not using the latest kernel.)</p>
<p>I learned how to overcome the issue from <a href="http://blog.alicegoldfuss.com/making-flamegraphs-with-containerized-java/">Making FlameGraphs with Containerized Java</a>. I just copied necessary files from the container to the host.</p>
<pre><code class="hljs sh"><span class="hljs-comment"># Horrible hack! Binaries to be used depend on your set up. `perf script` tells you what it wants if anything is missing.</span>
sudo docker cp mycontainer:/usr/bin/node /usr/bin/node
sudo docker cp mycontainer:/lib/ld-musl-x86_64.so.1 /lib/ld-musl-x86_64.so.1
sudo docker cp mycontainer:/usr/lib/libstdc++.so.6.0.22 /usr/lib/libstdc++.so.6.0.22</code></pre><p>To copy symbol map files, we need to find the <code>pid</code> in the container. We can do it by checking <code>/proc/${host_pid}/status</code>.</p>
<pre><code class="hljs ">$ cat /proc/21574/status | grep NSpid
NSpid:  21574   6
$ sudo docker cp mycontainer:/tmp/perf-6.map /tmp/perf-21574.map</code></pre><p>Now everything is ready! Then we can use <code>perf script</code> as usual.</p>
<pre><code class="hljs sh">sudo perf script &gt; stacks.<span class="hljs-variable">${pid}</span>.out</code></pre><p>I set up <a href="https://github.com/shuhei/perf-playground">a sample project</a> for profiling a Node.js application on Docker. It was nice to practice profiling a bit before doing it on production!</p>
<h2 id="conclusion">Conclusion</h2>
<p>Linux <code>perf</code> provides great observability to Node.js applications on production. Tools like CPU Flame Graph and FlameScope helped me to identify performance bottlenecks.</p>
<p>There are some gotchas to profile Node.js applications on Docker with Linux <code>perf</code>. It took some time for me to figure out how to do it because Node.js and Linux evolve day by day and I couldn&#39;t find many up-to-date resources online. I hope this post is helpful!</p>

        </div>
      </div>
    </div>
  
    
  
        </div>
        <footer class="footer">
          © Shuhei Kagawa
        </footer>
      </div>
    </body>
  </html>