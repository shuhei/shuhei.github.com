<?xml version="1.0" encoding="utf-8" ?><rss version="2.0" xmlns:atom="https://www.w3.org/2005/Atom"><channel><title>Shuhei Kagawa</title><link>https://shuheikagawa.com</link><atom:link href="https://shuheikagawa.com/blog/feed/rss.xml" rel="self" type="application/rss+xml"></atom:link><description>Shuhei Kagawa's blog</description><language>en-US</language><lastBuildDate>Tue, 31 Dec 2019 14:20:00 GMT</lastBuildDate><item><title>Winter Terminal (mostly Vim) Cleaning</title><link>https://shuheikagawa.com/blog/2019/12/31/winter-terminal-cleaning/</link><description><![CDATA[
<p>In December, I spent some time cleaning up my terminal setup. Dust had piled up in a year, and my terminal was getting slower. It was time to dust off.</p>
<p>Here are highlights of <a href="https://github.com/shuhei/dotfiles/compare/d5fa68a7514b040d0d19466ee85ebfbeb30b1d37...a8344b9d204af70f36ac8505df62425e87c5273d">the changes</a>.</p>
<h2 id="faster-text-rendering">Faster Text Rendering</h2>
<p>I noticed a non-negligible lag when I was editing JavaScript/TypeScript in Neovim. At first, I thought some Vim plugins caused it. But it was not true. Not only editing was slow, but also scrolling was slow. Text rendering itself was the problem.</p>
<p>I opened files of different types in Vim&#39;s vertical split and <code>less</code> in tmux&#39;s vertical split. And I scrolled down and (subjectively) evaluated the smoothness of scrolling.</p>
<p>It turned out that Vim was not the problem. With vertical splits of tmux, even <code>less</code> command was slow to scroll. Regardless of Vim or tmux, text rendering in vertical splits was slow on iTerm2. In retrospect, it makes sense because iTerm2 doesn&#39;t know about vertical split by Vim or tmux and can&#39;t limit rendering updates to the changed pane. <a href="https://www.iterm2.com/documentation-tmux-integration.html">iTerm2&#39;s tmux integration</a> may have helped, but I didn&#39;t try that.</p>
<p>I tried <a href="https://github.com/jwilm/alacritty">Alacritty</a>, and it was much faster! I had been using Alacritty before but switched back to iTerm2 for font ligatures. Now I didn&#39;t care much about font ligatures—ligatures look pretty, but glyphs for <code>!=</code> and <code>!==</code> confused me in JavaScript. So I switched to Alacritty again.</p>
<p>Also, I stopped using <a href="https://github.com/jordwalke/flatlandia">flatlandia</a> color scheme in Vim, and it improved the rendering speed a bit. I didn&#39;t dig into why, though.</p>
<h2 id="fzfvim">fzf.vim</h2>
<p><a href="https://github.com/junegunn/fzf.vim">fzf.vim</a> was a life changer. It provides a blazing fast incremental search for almost anything. I use it for file names (instead of <a href="https://github.com/kien/ctrlp.vim">ctrlp.vim</a>), commit history and grep. Especially, incremental grep with a preview is amazing.</p>
<h2 id="more-vim-cleaning">More Vim Cleaning</h2>
<ul>
<li><p>Started using <a href="https://github.com/dense-analysis/ale">ale</a> as a <a href="https://microsoft.github.io/language-server-protocol/">Language Server Protocol</a> client. I was using ale for linting and fixing, and <a href="https://github.com/autozimu/LanguageClient-neovim">LanguageClient-neovim</a> for LSP features. LanguageClient-neovim also shows a quickfix window when a file contains syntax errors and was conflicting with ale. I learned that ale supported LSP as well and made it handle LSP too.</p>
</li>
<li><p>Configured Vim to open <code>:help</code> in a vertical split. <code>:help</code> is a valuable resource when configuring Vim. The problem for me was that Vim opens help in a horizontal split by default. Opening help in a vertical split makes it much easier to read.</p>
<pre><code class="hljs vim"><span class="hljs-keyword">autocmd</span> FileType <span class="hljs-keyword">help</span> <span class="hljs-keyword">wincmd</span> H</code></pre></li>
<li><p>Sorted out JavaScript/JSX/TypeScript syntax highlighting. Vim sets <code>javascriptreact</code> to <code>.jsx</code> and <code>typescriptreact</code> to <code>.tsx</code> by default. But those file types don&#39;t work well with the plugin ecosystem because plugins for <code>javascript</code>/<code>typescript</code> file types don&#39;t work with <code>javascriptreact</code>/<code>typescriptreact</code> and popular JSX/TSX plugins use <code>javascript.jsx</code> and <code>typescript.tsx</code>.</p>
<pre><code class="hljs vim"><span class="hljs-keyword">autocmd</span> BufRead,BufNewFile *.jsx <span class="hljs-keyword">set</span> <span class="hljs-keyword">filetype</span>=javascript.jsx
<span class="hljs-keyword">autocmd</span> BufRead,BufNewFile *.tsx <span class="hljs-keyword">set</span> <span class="hljs-keyword">filetype</span>=typescript.tsx</code></pre></li>
<li><p>Stopped unnecessarily lazy-loading Vim plugins with <a href="https://github.com/Shougo/dein.vim">dein.vim</a>. I had configured file-type-specific plugins as lazy plugins of dein.vim without understanding much. The truth was that lazy plugins are meaningful only for plugins with <code>plugin</code> directory. Most of the file-type-specific plugins don&#39;t have <code>plugin</code> directory and are lazily loaded by default with <code>ftdetect</code> and <code>ftplugin</code>. <code>:echo dein#check_lazy_plugins()</code> shows those plugins that are ill-configured. I finally learned <a href="https://learnvimscriptthehardway.stevelosh.com/chapters/42.html">what those plugin directories do</a> after using Vim for several years...</p>
</li>
<li><p>Reviewed key mappings and removed waiting time by avoiding mappings that prefixed other mappings. For example, I had mappings of <code>,g</code> and <code>,gr</code>. <code>,g</code> was slow because Vim had to wait for a while to determine it was <code>,g</code> or <code>,gr</code>.</p>
</li>
<li><p>Tried Vim 8 but switched back to Neovim. Vim 8 worked well, but tiny details looked smoother in Neovim. For example, when syntax highlighting hangs up, Vim 8 hangs up while Neovim disables syntax highlighting and goes on.</p>
</li>
<li><p>Started documentation of my setup. I keep forgetting key mappings, useful plugins that I occasionally use, how things are set up, etc.</p>
</li>
</ul>

]]></description><pubDate>Tue, 31 Dec 2019 14:20:00 GMT</pubDate><guid isPermaLink="false">https://shuheikagawa.com/blog/2019/12/31/winter-terminal-cleaning/</guid></item><item><title>Generating Twitter Card Images from Blog Post Titles</title><link>https://shuheikagawa.com/blog/2019/10/13/generating-twitter-card-images/</link><description><![CDATA[
<p>Twitter shows links to some websites as nice cards with images, but not for all websites. I realized that Twitter didn&#39;t show the card for my blog. Why? It turned out that they were called Twitter Cards, and Twitter showed them for websites that provided specific metadata. Is it common sense? Maybe, but I didn&#39;t know.</p>
<p><a href="https://developer.twitter.com/en/docs/tweets/optimize-with-cards/overview/abouts-cards">Twitter Cards</a> give websites an ability to add an image, a video, etc. when they are shared on Twitter. A Twitter Card makes a tweet (physically) 3x more visible on the timeline. This post explains how I generated images from post titles using <a href="https://github.com/Automattic/node-canvas">node-canvas</a>, inspired by <a href="https://hatenablog.com/">Hatena Blog</a>.</p>
<p><img src="/images/twitter-card.png" alt="Twitter Card preview"></p>
<h2 id="meta-tags">Meta tags</h2>
<p>Twitter&#39;s bots look for <code>&lt;meta&gt;</code> tags in your page. If your page has a certain meta tags, it shows a Twitter Cards for links to the page. Check <a href="https://developer.twitter.com/en/docs/tweets/optimize-with-cards/overview/abouts-cards">the documentation</a> for more details. The <code>&lt;meta&gt;</code> tags look like these:</p>
<pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"twitter:card"</span> <span class="hljs-attr">content</span>=<span class="hljs-string">"summary_large_image"</span> /&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"twitter:site"</span> <span class="hljs-attr">content</span>=<span class="hljs-string">"@your_twitter_account"</span> /&gt;</span>

<span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"twitter:title"</span> <span class="hljs-attr">content</span>=<span class="hljs-string">"My Blog Post"</span> /&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"twitter:description"</span> <span class="hljs-attr">content</span>=<span class="hljs-string">"This is a blog post."</span> /&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">name</span>=<span class="hljs-string">"twitter:image"</span> <span class="hljs-attr">content</span>=<span class="hljs-string">"https://test.com/images/foo.png"</span> /&gt;</span></code></pre><p>Uh, they look a bit too platform-specific. <code>twitter:card</code> and <code>twitter:site</code> are specific to Twitter, but what about <code>twitter:title</code>, <code>twitter:description</code> and <code>twitter:image</code>? Twitter&#39;s bots also pick up Open Graph metadata tags, which are also used by other platforms like Facebook. So, we can use the <code>og:</code> tags instead of <code>twitter:</code> tags. Be careful that the attribute name of Open Graph metadata is <code>property</code> instead of <code>name</code>!</p>
<pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">property</span>=<span class="hljs-string">"og:title"</span> <span class="hljs-attr">content</span>=<span class="hljs-string">"My Blog Post"</span> /&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">property</span>=<span class="hljs-string">"og:description"</span> <span class="hljs-attr">content</span>=<span class="hljs-string">"This is a blog post."</span> /&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">property</span>=<span class="hljs-string">"og:image"</span> <span class="hljs-attr">content</span>=<span class="hljs-string">"https://test.com/images/foo.png"</span> /&gt;</span></code></pre><h2 id="homemade-static-site-generator">Homemade static site generator</h2>
<p><a href="https://github.com/shuhei/shuhei.github.com">My blog</a> is built with <a href="https://github.com/gulpjs/gulp">gulp</a> and some custom plugins and deployed to GitHub Pages. I started the blog with <a href="https://github.com/octopress/octopress">Octopress</a> several years ago and rewrote it with gulp when I was fascinated with gulp and JavaScript build tools. I once added React as a template engine and removed it later. Because of the history, its directory structure stays similar to the original one of Octopress. I write markdown files like <code>source/_posts/2019-10-13-foo.md</code> and the build system generates HTMLs like <code>/blog/2019/10/13/foo/index.html</code>.</p>
<p>To add Open Graph meta tags, I wrote a gulp plugin. Each gulp plugin is a transform stream that consumes and produces <a href="https://github.com/gulpjs/vinyl">vinyl</a> file objects. First, I made the plugin to extract image URLs from HTML and added necessary meta tags to the HTML template for <code>&lt;head&gt;</code> tag. Now, posts with at least one image got Twitter Cards.</p>
<h2 id="image-generation-and-text-wrapping">Image generation and text wrapping</h2>
<p>Most of my posts didn&#39;t have any images, while Twitter Cards don&#39;t look great without images. But I&#39;m too lazy to create an image for each blog post manually.</p>
<p>I found that <a href="https://hatenablog.com/">Hatena Blog</a>, a blogging platform in Japan, was <a href="https://twitter.com/search?q=%23%E3%81%AF%E3%81%A6%E3%81%AA%E3%83%96%E3%83%AD%E3%82%B0">generating images from blog post titles and descriptions</a>. It&#39;s a neat idea to promote blog posts without manual effort of blog authors. Can I replicate the image generation?</p>
<p>I found that many image-generation npm packages were using <a href="https://github.com/Automattic/node-canvas">node-canvas</a>. It provides the canvas API for Node.js and supports export options, including PNG. I decided to try that.</p>
<p>The canvas API was easy to use for me, but it doesn&#39;t provide text wrapping. I needed to come up with a way to break texts into lines. As <a href="https://stackoverflow.com/questions/2936112/text-wrap-in-a-canvas-element">a Q&amp;A on Stackoverflow</a> suggested, I used <code>ctx.measureText(text)</code> to measure the width of the text and remove words until the subtext fits the given width. And do the same for the remaining text.</p>
<p>The first line of this text wrapping algorithm is visualized as follows (it actually happens on the same line, but showing each try in its line for illustration):</p>
<p><img src="/images/twitter-card-image-line-break.png" alt="Wrapping text"></p>
<p>There were two edge cases to be covered. The first case is that a long word doesn&#39;t fit into the given width. The other case is that the text is split into too many lines, and they overflow the given height. I covered them by decreasing the font size until the entire text fits into the given rectangle.</p>
<p>The algorithm for the first edge case is visualized as follows (it tries smaller fonts until the word fits into the width):</p>
<p><img src="/images/twitter-card-image-font-sizes.png" alt="Try smaller font sizes"></p>
<p>I eventually came up with JavaScript code like this (<a href="https://github.com/shuhei/shuhei.github.com/blob/f30cb5cd85a4ef35a4fb73d94a01da44e03ae116/plugins/title-image.js">the full code is on GitHub</a>):</p>
<pre><code class="hljs js"><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">fitTextIntoRectangle</span>(<span class="hljs-params">{ ctx, text, maxFontSize, rect }</span>) </span>{
  <span class="hljs-comment">// Reduce font size until the title fits into the image.</span>
  <span class="hljs-keyword">for</span> (<span class="hljs-keyword">let</span> fontSize = maxFontSize; fontSize &gt; <span class="hljs-number">0</span>; fontSize -= <span class="hljs-number">1</span>) {
    ctx.font = getTitleFont(fontSize);
    <span class="hljs-keyword">let</span> words = text.split(<span class="hljs-string">" "</span>);
    <span class="hljs-keyword">let</span> { y } = rect;
    <span class="hljs-keyword">const</span> lines = [];
    <span class="hljs-keyword">while</span> (words.length &gt; <span class="hljs-number">0</span>) {
      <span class="hljs-keyword">let</span> i;
      <span class="hljs-keyword">let</span> size;
      <span class="hljs-keyword">let</span> subtext;
      <span class="hljs-comment">// Remove words until the rest fit into the width.</span>
      <span class="hljs-keyword">for</span> (i = words.length; i &gt;= <span class="hljs-number">0</span>; i -= <span class="hljs-number">1</span>) {
        subtext = words.slice(<span class="hljs-number">0</span>, i).join(<span class="hljs-string">" "</span>);
        size = ctx.measureText(subtext);

        <span class="hljs-keyword">if</span> (size.width &lt;= rect.width) {
          <span class="hljs-keyword">break</span>;
        }
      }

      <span class="hljs-keyword">if</span> (i &lt;= <span class="hljs-number">0</span>) {
        <span class="hljs-comment">// A word doesn't fit into a line. Try a smaller font size.</span>
        <span class="hljs-keyword">break</span>;
      }

      lines.push({
        <span class="hljs-attr">text</span>: subtext,
        <span class="hljs-attr">x</span>: rect.x,
        <span class="hljs-attr">y</span>: y + size.emHeightAscent
      });

      words = words.slice(i);
      y += size.emHeightAscent + size.emHeightDescent;
    }

    <span class="hljs-keyword">const</span> space = rect.y + rect.height - y;
    <span class="hljs-keyword">if</span> (words.length === <span class="hljs-number">0</span> &amp;&amp; space &gt;= <span class="hljs-number">0</span>) {
      <span class="hljs-comment">// The title fits into the image with the font size.</span>
      <span class="hljs-comment">// Vertically centering the text in the given rectangle.</span>
      <span class="hljs-keyword">const</span> centeredLines = lines.map(<span class="hljs-function"><span class="hljs-params">line</span> =&gt;</span> {
        <span class="hljs-keyword">return</span> {
          ...line,
          <span class="hljs-attr">y</span>: line.y + space / <span class="hljs-number">2</span>
        };
      });
      <span class="hljs-keyword">return</span> {
        fontSize,
        <span class="hljs-attr">lines</span>: centeredLines
      };
    }
  }

  <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">Error</span>(
    <span class="hljs-string">`Text layout failed: The given text '<span class="hljs-subst">${text}</span>' did not fit into the given rectangle <span class="hljs-subst">${<span class="hljs-built_in">JSON</span>.stringify(
      rect
    )}</span> even with the smallest font size (1)`</span>
  );
}</code></pre><h2 id="font">Font</h2>
<p>My website is using <a href="https://fonts.google.com/specimen/IBM+Plex+Sans">IBM Plex Sans</a> via Google Fonts. I wanted to use the same font in the images. Fortunately, node-canvas provides an API to load fonts, and the font is available also on npm.</p>
<pre><code class="hljs sh">yarn add -D @ibm/plex</code></pre><pre><code class="hljs js"><span class="hljs-keyword">const</span> { registerFont } = <span class="hljs-built_in">require</span>(<span class="hljs-string">"canvas"</span>);

registerFont(
  <span class="hljs-string">"./node_modules/@ibm/plex/IBM-Plex-Sans/fonts/complete/otf/IBMPlexSans-Bold.otf"</span>,
  {
    <span class="hljs-attr">family</span>: <span class="hljs-string">"IBM Plex Sans"</span>,
    <span class="hljs-attr">weight</span>: <span class="hljs-string">"bold"</span>
  }
);

<span class="hljs-comment">// ...</span>

ctx.font = <span class="hljs-string">"bold 30px 'IBM Plex Sans'"</span>;</code></pre><h2 id="done">Done!</h2>
<p>So, the feature is done. It looked trivial at first glance, but the text wrapping algorithm was fun to write. Now I got to write more blog posts to use this feature!</p>

]]></description><pubDate>Sun, 13 Oct 2019 14:59:00 GMT</pubDate><guid isPermaLink="false">https://shuheikagawa.com/blog/2019/10/13/generating-twitter-card-images/</guid></item><item><title>Migrating from bash to zsh</title><link>https://shuheikagawa.com/blog/2019/10/08/migrating-from-bash-to-zsh/</link><description><![CDATA[
<p>A few days ago, I updated my Macbook Air to macOS Catalina. The installation took some time, but it was done when I got up the next morning. The applications that I use seemed to work fine on Catalina. But bash started complaining at the beginning of new sessions.</p>
<pre><code class="hljs console">The default interactive shell is now zsh.
To update your account to use zsh, please run `chsh -s /bin/zsh`.
For more details, please visit https://support.apple.com/kb/HT208050.</code></pre><p>I asked whether I should migrate to zsh on Twitter. Three people said &quot;yes&quot; as if it was common sense. OK, let&#39;s migrate.</p>
<h2 id="changing-the-default-shell-of-tmux">Changing the default shell of tmux</h2>
<p>First, I followed the instruction from Apple.</p>
<pre><code class="hljs sh">chsh -s /bin/zsh</code></pre><p>However, it didn&#39;t change the default shell of tmux. I restarted sessions in tmux, and restarted iTerm 2 and the tmux server. But tmux still started bash sessions. Why?</p>
<p>I googled. There was <a href="https://superuser.com/questions/253786/how-can-i-make-tmux-use-my-default-shell">a Q&amp;A for the exact problem</a> on superuser. The <code>default-command</code> option of tmux is the default shell. I had a hardcoded <code>bash</code> there! By the way, <code>reattach-to-user-namespace</code> is for sharing Mac&#39;s clipboard with tmux.</p>
<pre><code class="hljs ">set-option -g default-command &quot;reattach-to-user-namespace -l bash&quot;</code></pre><p>I updated it with <code>SHELL</code> environment variable so that I can migrate to any shell in the future!</p>
<pre><code class="hljs ">set-option -g default-command &quot;reattach-to-user-namespace -l ${SHELL}&quot;</code></pre><h2 id="command-prompt">Command prompt</h2>
<p>Then I installed <a href="https://github.com/robbyrussell/oh-my-zsh">oh-my-zsh</a> and copied my <code>.bash_profile</code> to <code>.zshrc</code>. Most of the content of my <code>.bash_profile</code> were aliases and <code>PATH</code>s. They worked fine on zsh too.</p>
<p>But zsh has a different format for prompt. oh-my-zsh provides a lot of nice prompt themes, but I wanted to keep using the one that I had configured with bash. Let&#39;s migrate it to zsh.</p>
<p><del>oh-my-zsh has a directory for custom themes (<code>.oh-my-zsh/custom/themes</code>). I moved the <code>custom</code> directory to <a href="https://github.com/shuhei/dotfiles">my dotfiles repo</a> and symlinked it so that I can manage my custom theme with Git without forking oh-my-zsh itself.</del> [Update on Oct 24, 2019] I realized that this symlink approach prevents updates of oh-my-zsh because it modifies the files in the git local clone of oh-my-zsh. <a href="https://github.com/robbyrussell/oh-my-zsh/wiki/Customization">The official customization guide</a> recommends to use <code>ZSH_CUSTOM</code> variable to specify the location of a custom directory. Now I&#39;m using <code>ZSH_CUSTOM</code> to point to a directory in my dotfiles repo.</p>
<p>Eventually, I came up with a theme like this:</p>
<p><img src="/images/zsh_prompt.png" alt="my custom theme"></p>
<pre><code class="hljs bash">ZSH_THEME_GIT_PROMPT_PREFIX=<span class="hljs-string">"%{<span class="hljs-variable">$fg</span>[white]%}("</span>
ZSH_THEME_GIT_PROMPT_SUFFIX=<span class="hljs-string">"%{<span class="hljs-variable">$fg</span>[white]%})%{<span class="hljs-variable">$reset_color</span>%}"</span>
ZSH_THEME_GIT_PROMPT_DIRTY=<span class="hljs-string">"*"</span>
ZSH_THEME_GIT_PROMPT_CLEAN=<span class="hljs-string">""</span>

<span class="hljs-comment"># %~ is the current working directory relative to the home directory</span>
PROMPT=<span class="hljs-string">'[$FG[228]%~%{$reset_color%}]'</span>
PROMPT+=<span class="hljs-string">' $(git_prompt_info)'</span>
PROMPT+=<span class="hljs-string">' %(?.$FG[154].$FG[009])€%{$reset_color%} '</span></code></pre><p>Each oh-my-zsh theme defines a variable called <code>PROMPT</code>. Aside from <a href="http://zsh.sourceforge.net/Doc/Release/Prompt-Expansion.html">its syntax</a>, I was not sure how and when <code>PROMPT</code> was evaluated. In hindsight, it is a string that is built once when a session starts or <code>source .zshrc</code>. Every time a prompt is shown, <code>PROMPT</code> is evaluated, meaning escapes (starting with <code>%</code>) and variables in it are expanded.</p>
<h3 id="colors">Colors</h3>
<p>At the beginning, I was baffled by how to specify colors. For example, the following <code>PROMPT</code> shows &quot;some red text&quot; in red.</p>
<pre><code class="hljs bash">PROMPT=<span class="hljs-string">'%{$fg[red]%}some red text%{$reset_color%}'</span></code></pre><p><code>$fg[red]</code> has the code that makes its following text red. <code>$reset_color</code> has the code that resets the color. The tricky part is that these codes need to be surrounded by <code>%{</code> and <code>%}</code> in <code>PROMPT</code>.</p>
<p><a href="https://github.com/zsh-users/zsh/blob/243e46998eb29665ec345e531b2d1bb6921ed578/Functions/Misc/colors#L97-L117">zsh provides handy variables for colors</a>.</p>
<ul>
<li><code>reset_color</code></li>
<li><code>fg</code>, <code>fg_bold</code>, <code>fg_no_bold</code>: They are associative arrays (similar to JavaScript objects).</li>
<li><code>bg</code>, <code>bg_bold</code>, <code>bg_no_bold</code></li>
</ul>
<p>Also, <a href="https://github.com/robbyrussell/oh-my-zsh/blob/b09aed9cc7e2099f3e7f2aa2632660bc510f3e35/lib/spectrum.zsh">oh-my-zsh provides 256 colors</a>.</p>
<ul>
<li><code>FX</code>: This has codes for text effects like <code>FX[underline]</code>.</li>
<li><code>FG</code>: 256 colors for foreground like <code>FG[102]</code>.</li>
<li><code>BG</code>: 256 colors for background like <code>BG[123]</code>.</li>
</ul>
<p><code>spectrum_ls</code> and <code>spectrum_bls</code> commands show you all the 256 colors! Note that values in <code>FX</code>, <code>FG</code> and <code>BG</code> are already surrounded by <code>%{</code> and <code>%}</code>, and we don&#39;t need to do it again.</p>
<p>We can examine those variables in the terminal.</p>
<pre><code class="hljs sh"><span class="hljs-built_in">echo</span> <span class="hljs-string">"<span class="hljs-variable">${fg[yellow]}</span>hello<span class="hljs-variable">${reset_color}</span> <span class="hljs-variable">${bg[green]}</span>world<span class="hljs-variable">${reset_color}</span>"</span>

<span class="hljs-comment"># `(kv)` extracts key values from an associative array.</span>
<span class="hljs-built_in">echo</span> <span class="hljs-variable">${(kv)fg}</span>
<span class="hljs-built_in">echo</span> <span class="hljs-variable">${(kv)FG}</span></code></pre><h3 id="exit-code">Exit code</h3>
<p>With bash, <a href="/blog/2015/10/18/color-prompt-by-exit-code/">I had a trick to change the color of the prompt by the previous command&#39;s exit code</a>. How can I achieve this with zsh?</p>
<p><img src="/images/exit_code.png" alt="Change color by exit code"></p>
<p>Surprisingly, <a href="https://stackoverflow.com/questions/4466245/customize-zshs-prompt-when-displaying-previous-command-exit-code">zsh prompt expression has a special syntax for switching prompt by exit code</a>. To be accurate, it&#39;s a combination of a ternary operator and <code>?</code> for exit code check.</p>
<pre><code class="hljs bash"><span class="hljs-comment"># Shows "foo" if the exit code is 0 and "bar" if the exit code is non-zero.</span>
%(?.foo.bar)</code></pre><p>The following expression shows the Euro sign in green if the exit code is 0 and in red if the exit code is non-zero.</p>
<pre><code class="hljs bash">%(?.%{<span class="hljs-variable">$fg</span>[green]%}.%{<span class="hljs-variable">$fg</span>[red]%})€%{<span class="hljs-variable">$reset_color</span>%}</code></pre><h3 id="git-info">Git info</h3>
<p><code>git_prompt_info()</code> function outputs git info such as the branch name and the state of the working tree (clean or dirty). We can customize its output by <code>ZSH_THEME_GIT_PROMPT_*</code> variables.</p>
<p>I wrote something like this:</p>
<pre><code class="hljs bash">ZSH_THEME_GIT_PROMPT_PREFIX=<span class="hljs-string">"%{<span class="hljs-variable">$fg</span>[white]%}("</span>
ZSH_THEME_GIT_PROMPT_SUFFIX=<span class="hljs-string">"%{<span class="hljs-variable">$fg</span>[white]%})%{<span class="hljs-variable">$reset_color</span>%}"</span>
ZSH_THEME_GIT_PROMPT_DIRTY=<span class="hljs-string">"*"</span>
ZSH_THEME_GIT_PROMPT_CLEAN=<span class="hljs-string">""</span>

PROMPT=<span class="hljs-string">"... <span class="hljs-variable">$(git_prompt_info)</span> ..."</span></code></pre><p>I thought it was done and went back to work. But when I switched the git branch, the prompt stayed the same. Why? I googled again. There was <a href="https://github.com/robbyrussell/oh-my-zsh/issues/4826">an issue</a> for the same problem. The <code>PROMPT</code> needs to be created with single quotes instead of double quotes so that dynamic parts are not evaluated when it&#39;s defined!</p>
<pre><code class="hljs bash">PROMPT=<span class="hljs-string">'... $(git_prompt_info) ...'</span></code></pre><h2 id="conclusion">Conclusion</h2>
<p>I have migrated my terminal from bash to zsh. My initial motivation was passive (Catalina deprecated bash), but it&#39;s always fun to try something new (to me). I&#39;m looking forward to trying cool zsh plugins and tricks!</p>

]]></description><pubDate>Wed, 09 Oct 2019 21:20:00 GMT</pubDate><guid isPermaLink="false">https://shuheikagawa.com/blog/2019/10/08/migrating-from-bash-to-zsh/</guid></item><item><title>Writing an Interpreter and a Compiler in Rust</title><link>https://shuheikagawa.com/blog/2019/10/06/interpreter-and-compiler-in-rust/</link><description><![CDATA[
<p>In the spring of this year, I read <a href="https://interpreterbook.com/">Writing an Interpreter in Go</a> and <a href="https://compilerbook.com/">Writing a Compiler in Go</a> by <a href="https://thorstenball.com/">Thorsten Ball</a>, and implemented <a href="https://github.com/shuhei/cymbal">an interpreter and a compiler</a> from the books in Rust. (I started writing this post in April but left unfinished for six months. Now I&#39;m finishing it.)</p>
<p>The first book <a href="https://interpreterbook.com/">Writing an Interpreter in Go</a> is about writing a parser and an interpreter for a programming language called Monkey. Monkey&#39;s feature set is limited, but it has some interesting features that modern programming languages have—such as function as a first-class citizen and closures.</p>
<pre><code class="hljs js"><span class="hljs-keyword">let</span> fibonacci = fn(x) {
    <span class="hljs-keyword">if</span> (x == <span class="hljs-number">0</span>) {
        <span class="hljs-number">0</span>
    } <span class="hljs-keyword">else</span> {
        <span class="hljs-keyword">if</span> (x == <span class="hljs-number">1</span>) {
            <span class="hljs-number">1</span>
        } <span class="hljs-keyword">else</span> {
            fibonacci(x - <span class="hljs-number">1</span>) + fibonacci(x - <span class="hljs-number">2</span>)
        }
    }
};
fibonacci(<span class="hljs-number">15</span>);</code></pre><p>The second book <a href="https://compilerbook.com/">Writing a Compiler in Go</a> taught me to write a simple compiler and a simple virtual machine. The compiler compiles Monkey scripts into instructions (and constants), and the virtual machine executes the instructions. For example, an expression <code>1 + 2</code> is compiled into:</p>
<pre><code class="hljs rs"><span class="hljs-comment">// Constants</span>
<span class="hljs-built_in">vec!</span>[
    Object::Integer(<span class="hljs-number">1</span>),
    Object::Integer(<span class="hljs-number">2</span>),
]

<span class="hljs-comment">// Instructions</span>
<span class="hljs-built_in">vec!</span>[
    make_u16(OpCode::Constant, <span class="hljs-number">0</span>),
    make_u16(OpCode::Constant, <span class="hljs-number">1</span>),
    make(OpCode::Add),
    make(OpCode::Pop),
]</code></pre><h2 id="how-i-started">How I started</h2>
<p>I had bought <em>Writing an Interpreter in Go</em> more in 2017, but it had been sleeping in my bookshelf (<a href="https://en.wikipedia.org/wiki/Tsundoku">Tsundoku</a>). Recently, I wanted to relearn a little Go for work. I took the book from my bookshelf and started following the book—typing the code in Go. I did two chapters, and new Go syntaxes stopped appearing. I achieved my initial purpose—relearning Go—earlier than I thought because the book used a limited set of Go&#39;s language features. Then Rust came to my mind.</p>
<p>Before starting this project, I had written two simple command-line tools with Rust (<a href="https://github.com/shuhei/colortty">colortty</a> and <a href="https://github.com/shuhei/ynan26">ynan26</a>), but they were too small to learn different aspects of Rust. I wanted to learn more by implementing something not trivial.</p>
<h2 id="good-things-about-rust">Good things about Rust</h2>
<p>First, I rewrote what I had written in Go with Rust and continued the rest of the book. The implementation in Rust was less redundant than the one in Go. Also, it was more type-safe thanks to <code>enum</code>s and <code>Result</code>. Especially <code>enum</code>s were perfect for AST (Abstract Syntax Tree) and evaluated objects.</p>
<pre><code class="hljs rs"><span class="hljs-comment">// An example of AST</span>
<span class="hljs-meta">#[derive(Debug, PartialEq, Clone, Hash, Eq)]</span>
<span class="hljs-keyword">pub</span> <span class="hljs-class"><span class="hljs-keyword">enum</span> <span class="hljs-title">Expression</span></span> {
    Identifier(<span class="hljs-built_in">String</span>),
    IntegerLiteral(<span class="hljs-built_in">i64</span>),
    StringLiteral(<span class="hljs-built_in">String</span>),
    Boolean(<span class="hljs-built_in">bool</span>),
    Array(<span class="hljs-built_in">Vec</span>&lt;Expression&gt;),
    Hash(HashLiteral),
    Index(<span class="hljs-built_in">Box</span>&lt;Expression&gt;, <span class="hljs-built_in">Box</span>&lt;Expression&gt;),
    Prefix(Prefix, <span class="hljs-built_in">Box</span>&lt;Expression&gt;),
    Infix(Infix, <span class="hljs-built_in">Box</span>&lt;Expression&gt;, <span class="hljs-built_in">Box</span>&lt;Expression&gt;),
    If(<span class="hljs-built_in">Box</span>&lt;Expression&gt;, BlockStatement, <span class="hljs-built_in">Option</span>&lt;BlockStatement&gt;),
    FunctionLiteral(<span class="hljs-built_in">Vec</span>&lt;<span class="hljs-built_in">String</span>&gt;, BlockStatement),
    Call(<span class="hljs-built_in">Box</span>&lt;Expression&gt;, <span class="hljs-built_in">Vec</span>&lt;Expression&gt;),
}</code></pre><p>However, harder parts came later when the compiler and the virtual machine grew complex.</p>
<h2 id="nested-symbol-tables-were-a-linked-list">Nested symbol tables were a linked list</h2>
<p>To implement nested scopes, the Compiler Book uses self-recursive <code>struct</code>s for nested symbol tables. I was struggling with their ownership. I tried <code>Rc</code> and <code>RefCell</code>, but still was not able to get through them.</p>
<p>Then, I went to Rust Hack and Learn—a local meetup at Mozilla Berlin office—and asked how to get over ownership rules. One person (sorry, I didn&#39;t ask his name!) recommended me a book <a href="https://rust-unofficial.github.io/too-many-lists/">Learn Rust With Entirely Too Many Linked Lists </a>.</p>
<p>The book introduces several versions of linked list implementations in Rust even though its precaution is not to implement linked lists in Rust. It had some techniques that I had recently learned, and much more. After a while, I realized that I had been trying to implement a linked list. Then I changed the self-recursive <code>struct</code> to a <code>Vec</code>, and it solved most of my headaches. So, the book&#39;s precaution was right. Don&#39;t implement a liked list.</p>
<pre><code class="hljs rs"><span class="hljs-comment">// Before</span>
<span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">SymbolTable</span></span> {
  store: HashMap&lt;<span class="hljs-built_in">String</span>, Symbol&gt;,

  <span class="hljs-comment">// This is a linked list!</span>
  outer: <span class="hljs-built_in">Option</span>&lt;SymbolTable&gt;;
}

<span class="hljs-comment">// After</span>
<span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">SymbolLayer</span></span> {
  store: HashMap&lt;<span class="hljs-built_in">String</span>, Symbol&gt;,
}

<span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">SymbolTable</span></span> {
  current: SymbolLayer;
  outers: <span class="hljs-built_in">Vec</span>&lt;SymbolStore&gt;;
}</code></pre><h2 id="i-learned-basics-of-how-programs-work-at-low-level">I learned basics of how programs work at low-level</h2>
<p>Even before starting the project, I had some vague ideas about parser, interpreter and compiler thanks to my previous projects. But I hadn&#39;t had concrete ideas about compilers, especially about how to translate high-level code like function calls and closures into low-level instructions. After the project, now I can confidently say what is on the stack and what is on the heap.</p>
<p>Also, the knowledge about stack was useful to understand some of the concepts of Rust itself. Rust&#39;s compiler to know the sizes of types because it needs to generate machine code that allocates values of the types on the stack.</p>
<h2 id="conclusion">Conclusion</h2>
<p>It was a fun project. I learned something, but there is much more to learn in Rust. Also, now I can admire modern interpreters and compilers like V8 more than before.</p>
<p><a href="https://interpreterbook.com/">Writing an Interpreter in Go</a> and <a href="https://compilerbook.com/">Writing a Compiler in Go</a> are great. I liked their hands-on approach with many unit tests.</p>

]]></description><pubDate>Sun, 06 Oct 2019 21:32:00 GMT</pubDate><guid isPermaLink="false">https://shuheikagawa.com/blog/2019/10/06/interpreter-and-compiler-in-rust/</guid></item><item><title>DNS Polling for Reliability</title><link>https://shuheikagawa.com/blog/2019/04/30/dns-polling/</link><description><![CDATA[
<p>In December 2018, I wrote a package to poll and cache DNS records, <a href="https://github.com/shuhei/pollen">pollen</a>, as a mitigation for incidents at work.</p>
<p>My team at work migrated our Node.js servers from AWS EC2 C4 instances to C5 instances. Then mysterious timeout errors on outbound HTTP(S) calls started happening. They happened only in an availability zone at a time. We tried different things to investigate the issue, like profiling and <code>tcpdump</code>, but couldn&#39;t find the cause. Eventually, AWS Support suggested that the incidents were correlated to DNS timeouts in their metrics. According to them, C5 instances don&#39;t retry DNS lookups while C4 instances do.</p>
<h2 id="nodejs-is-vulnerable-to-dns-failures">Node.js is Vulnerable to DNS Failures</h2>
<p>In the microservice world, we work hard to make remote procedure calls (with HTTPS) reliable. We use timeout, retry, fallback, etc. to make it as reliable as possible. However, we hadn&#39;t paid enough attention to DNS lookup, which we use for service discovery. It can easily be a single point of failure because we can&#39;t call servers without knowing their IP addresses.</p>
<p>Node.js is especially vulnerable to DNS lookup failures because:</p>
<ol>
<li>Node.js standard library doesn&#39;t have DNS cache by default while other languages/runtimes like Java and Go have it by default.</li>
<li>Node.js uses a small thread pool to make DNS lookups. When there are slow DNS queries or packet loss, subsequent DNS lookups need to wait for them to finish or timeout.<ul>
<li>Before <a href="https://github.com/nodejs/node/pull/22997">Node 10.12.0</a>, it was even worse because slow DNS queries affected other tasks in the threadpool like file IO and GZIP encoding/decoding.</li>
</ul>
</li>
</ol>
<h2 id="caching-at-os-level">Caching at OS-level</h2>
<p>We can make DNS lookups fast and reliable by caching it. <a href="https://github.com/nodejs/node/issues/5893">An issue on the nodejs/node repo</a> recommends to have caching at OS-level. We can run a daemon like dnsmasq, unbound, CoreDNS, etc.</p>
<p>However, it&#39;s not always easy depending on the platform that you are using. My team was using <a href="https://stups.io/">a platform where we just deploy your application Docker container</a>, and it was hard to set up another daemon on the OS. The majority of the users of the platform were application runtimes such as Java and Go, which have basic DNS caching by default and rarely have the same issues with Node.js applications. It was hard to convince the platform team to introduce per-node DNS caching to the platform only for Node.js applications without a concrete evidence while they were focusing on a new Kubernetes-based platform. (They eventually added per-node DNS caching to the new platform later, but the application in question won&#39;t move to it because of reasons...)</p>
<p>Because the incidents didn&#39;t happen on C4 instances and we had other priorities to work on, we just rolled back and kept using C4 instances for a while. However, I wanted to finish the issue before celebrating 2019. So, I decided to implement DNS caching on the application layer with Node.js.</p>
<h2 id="dns-caching-and-prefetching-with-nodejs">DNS Caching and Prefetching with Node.js</h2>
<p>There were already some DNS caching packages:</p>
<ul>
<li><a href="https://github.com/yahoo/dnscache">yahoo/dnscache</a></li>
<li><a href="https://github.com/eduardbcom/lookup-dns-cache">eduardbcom/lookup-dns-cache</a></li>
</ul>
<p>The packages looked great, but there was an edge case that they didn&#39;t cover. Both of the packages throw away caches after some time (<code>dnscache</code> uses <code>ttl</code> option and <code>lookup-dns-cache</code> uses the TTL that DNS servers return) and make DNS lookups again. This poses a risk where HTTP requests fail if DNS servers are down at the time.</p>
<p>To avoid making DNS lookups on demand, we can prefetch DNS records and always provide cached DNS records. This means that we may get outdated IP addresses. However, DNS records didn&#39;t change often for my case. I thought it would be better to use expired DNS records than just giving up. In the worst case, we would get an SSL certificate error if the expired IP addresses point to wrong servers as long as we use HTTPS.</p>
<h2 id="http-keep-alive-persistent-connection">HTTP Keep-Alive (Persistent Connection)</h2>
<p>There was another issue that I wanted to solve with this package: keeping HTTP Keep-Alive connections as long as possible.</p>
<p>We have been using HTTP Keep-Alive for good performance. However, we couldn&#39;t keep the Keep-Alive connections forever because our backend servers may change their IP addresses (DNS-based traffic switch in our case). To avoid keeping stale connections, we were re-creating TCP/TLS connections for each minute, by rotating HTTP agents and later using the <code>activeSocketTTL</code> option of <code>keepaliveagent</code>. However, this is not optimal because IP addresses don&#39;t change most of the time.</p>
<p>The DNS caching and prefetching tell us when IP addresses change. So we can keep using existing connections as long as IP addresses stay same and re-connect only when IP addresses change. In this way, we can avoid unnecessary TCP/TLS handshakes.</p>
<h2 id="result">Result</h2>
<p>I wrote <a href="https://github.com/shuhei/pollen">pollen</a>, tested it with C4 instances and migrated our servers to C5 again. No issues happened after five months. So, it seems that DNS failure was the cause and the package can mitigate it.</p>
<p>I had expected performance improvement because of fewer TCP/TLS handshakes, but I didn&#39;t find much difference in latency.</p>
<h2 id="how-to-use-it">How to Use It</h2>
<pre><code class="hljs sh">npm i -S @shuhei/pollen
<span class="hljs-comment"># or</span>
yarn add @shuhei/pollen</code></pre><pre><code class="hljs js"><span class="hljs-keyword">const</span> https = <span class="hljs-built_in">require</span>(<span class="hljs-string">'https'</span>);
<span class="hljs-keyword">const</span> { DnsPolling, HttpsAgent } = <span class="hljs-built_in">require</span>(<span class="hljs-string">'@shuhei/pollen'</span>);

<span class="hljs-keyword">const</span> dnsPolling = <span class="hljs-keyword">new</span> DnsPolling({
  <span class="hljs-attr">interval</span>: <span class="hljs-number">30</span> * <span class="hljs-number">1000</span> <span class="hljs-comment">// 30 seconds by default</span>
});
<span class="hljs-comment">// Just a thin wrapper of https://github.com/node-modules/agentkeepalive</span>
<span class="hljs-comment">// It accepts all the options of `agentkeepalive`.</span>
<span class="hljs-keyword">const</span> agent = <span class="hljs-keyword">new</span> HttpsAgent();

<span class="hljs-keyword">const</span> hostname = <span class="hljs-string">'shuheikagawa.com'</span>;
<span class="hljs-keyword">const</span> req = https.request({
  hostname,
  <span class="hljs-attr">path</span>: <span class="hljs-string">'/'</span>,
  <span class="hljs-comment">// Make sure to call `getLookup()` for each request!</span>
  <span class="hljs-attr">lookup</span>: dnsPolling.getLookup(hostname),
  agent,
});</code></pre><h2 id="bonus-dns-lookup-metrics">Bonus: DNS Lookup Metrics</h2>
<p>Because DNS lookup is a critical operation, it is a good idea to monitor its rate, errors and latency. <code>pollen</code> emits events for this purpose.</p>
<pre><code class="hljs js">dnsPolling.on(<span class="hljs-string">'resolve:success'</span>, ({ hostname, duration, update }) =&gt; {
  <span class="hljs-comment">// Hypothetical functions to update metrics...</span>
  recordDnsLookup();
  recordDnsLatency(duration);

  <span class="hljs-keyword">if</span> (update) {
    logger.info({ hostname, duration }, <span class="hljs-string">'IP addresses updated'</span>);
  }
});
dnsPolling.on(<span class="hljs-string">'resolve:error'</span>, ({ hostname, duration, error }) =&gt; {
  <span class="hljs-comment">// Hypothetical functions to update metrics...</span>
  recordDnsLookup();
  recordDnsLatency(duration);
  recordDnsError();

  logger.warn({ hostname, <span class="hljs-attr">err</span>: error, duration }, <span class="hljs-string">'DNS lookup error'</span>);
});</code></pre><p>I was surprised by DNS lookups occasionally taking 1.5 seconds. It might be because of retries of <a href="https://c-ares.haxx.se/">c-ares</a>, but I&#39;m not sure yet (<a href="https://c-ares.haxx.se/ares_init_options.html">its default timeout seems to be 5 seconds...</a>).</p>
<p>Because <code>pollen</code> makes fewer DNS lookups, the events don&#39;t happen frequently. I came across an issue of histogram implementation that greatly skewed percentiles of infrequent events, and started using HDR histograms. Check out <a href="/blog/2018/12/29/histogram-for-time-series-metrics-on-node-js/">Histogram for Time-Series Metrics on Node.js</a> for more details.</p>
<p>Even if you don&#39;t use <code>pollen</code>, it is a good idea to monitor DNS lookups.</p>
<pre><code class="hljs js"><span class="hljs-keyword">const</span> dns = <span class="hljs-built_in">require</span>(<span class="hljs-string">'dns'</span>);

<span class="hljs-keyword">const</span> lookupWithMetrics = <span class="hljs-function">(<span class="hljs-params">hostname, options, callback</span>) =&gt;</span> {
  <span class="hljs-keyword">const</span> cb = callback || options;
  <span class="hljs-keyword">const</span> startTime = <span class="hljs-built_in">Date</span>.now();

  <span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">onLookup</span>(<span class="hljs-params">err, address, family</span>) </span>{
    <span class="hljs-keyword">const</span> duration = <span class="hljs-built_in">Date</span>.now() - startTime;
    cb(err, address, family);

        <span class="hljs-comment">// Hypothetical functions to update metrics...</span>
    recordDnsLookup();
    recordDnsLatency(duration);
    <span class="hljs-keyword">if</span> (err) {
      recordDnsError();
      logger.warn({ hostname, err, duration }, <span class="hljs-string">'DNS lookup error'</span>);
    }
  }

  <span class="hljs-keyword">return</span> dns.lookup(hostname, options, onLookup);
};

<span class="hljs-keyword">const</span> req = https.request({
  ...,
  <span class="hljs-attr">lookup</span>: lookupWithMetrics
});</code></pre><h2 id="conclusion">Conclusion</h2>
<p>Give <a href="https://github.com/shuhei/pollen">pollen</a> a try if you are:</p>
<ul>
<li>seeing DNS timeouts on outbound API calls</li>
<li>using DNS for service discovery</li>
<li>running your Node.js servers without DNS caching</li>
</ul>
<p>Also, don&#39;t forget to monitor DNS lookups!</p>

]]></description><pubDate>Mon, 29 Apr 2019 22:14:00 GMT</pubDate><guid isPermaLink="false">https://shuheikagawa.com/blog/2019/04/30/dns-polling/</guid></item><item><title>Check Your server.keepAliveTimeout</title><link>https://shuheikagawa.com/blog/2019/04/25/keep-alive-timeout/</link><description><![CDATA[
<p>One of my Node.js server applications at work had constant 502 errors at AWS ELB (Application Load Balancer) in front of it (<code>HTTPCode_ELB_502_Count</code>). The number was very small. It was around 0.001% of the entire requests. It was not happening on other applications with the same configuration but with shorter response times and more throughputs. Because of the low frequency, I hadn&#39;t bothered investigating it for a while.</p>
<pre><code class="hljs ">clients -&gt; AWS ELB -&gt; Node.js server</code></pre><p>I recently came across a post, <a href="https://medium.com/@liquidgecka/a-tale-of-unexpected-elb-behavior-5281db9e5cb4">A tale of unexpected ELB behavior.</a> It says ELB pre-connects to backend servers, and it can cause a race condition where ELB thinks a connection is open, but the backend closes it. It clicked my memory about the ELB 502 issue. After some googling, I found <a href="https://blog.percy.io/tuning-nginx-behind-google-cloud-platform-http-s-load-balancer-305982ddb340">Tuning NGINX behind Google Cloud Platform HTTP(S) Load Balancer</a>. It describes an issue on GCP Load Balancer and NGINX, but its takeaway was to have the server&#39;s keep alive idle timeout longer than the load balancer&#39;s timeout. This advice seemed applicable even to AWS ELB and Node.js server.</p>
<p>According to <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#connection-idle-timeout">AWS documentation</a>, Application Load Balancer has 60 seconds of connection idle timeout by default. It also suggests:</p>
<blockquote>
<p>We also recommend that you configure the idle timeout of your application to be larger than the idle timeout configured for the load balancer.</p>
</blockquote>
<p><a href="https://nodejs.org/api/http.html#http_server_keepalivetimeout">Node.js <code>http</code>/<code>https</code> server has 5 seconds keep alive timeout by default</a>. I wanted to make it longer. With <a href="https://expressjs.com/">Express</a>, we can do it like the following:</p>
<pre><code class="hljs js"><span class="hljs-keyword">const</span> express = <span class="hljs-built_in">require</span>(<span class="hljs-string">"express"</span>);

<span class="hljs-keyword">const</span> app = express();
<span class="hljs-comment">// Set up the app...</span>
<span class="hljs-keyword">const</span> server = app.listen(<span class="hljs-number">8080</span>);

server.keepAliveTimeout = <span class="hljs-number">61</span> * <span class="hljs-number">1000</span>;</code></pre><p>And the ELB 502 errors disappeared!</p>
<p>As hindsight, there was already <a href="https://adamcrowder.net/posts/node-express-api-and-aws-alb-502/">Dealing with Intermittent 502&#39;s between an AWS ALB and Express Web Server</a> on the internet, which describes exactly the same issue with more details. (I found it while writing this post...) Also, the same issue seems to be happening with different load balancers/proxies and different servers. Especially the 5-second timeout of Node.js is quite short and prone to this issue. I found that it had happened with a reverse proxy (<a href="https://github.com/zalando-incubator/kube-ingress-aws-controller">Skipper as k8s ingress</a>) and another Node.js server at work. I hope this issue becomes more widely known.</p>
<h2 id="update-on-april-29-2019">Update on April 29, 2019</h2>
<p>Oleksii told me in a comment that only <code>server.keepAliveTimeout</code> was not enough on Node.js 10.15.2. It turned out that we also need to configure <code>server.headersTimeout</code> longer than <code>server.keepAliveTimeout</code> on Node.js 10.15.2 and newer. See <a href="https://github.com/nodejs/node/issues/27363">his issue on GitHub</a> for more details. Thanks, Oleksii!</p>
<pre><code class="hljs js">server.keepAliveTimeout = <span class="hljs-number">61</span> * <span class="hljs-number">1000</span>;
server.headersTimeout = <span class="hljs-number">65</span> * <span class="hljs-number">1000</span>; <span class="hljs-comment">// This should be bigger than `keepAliveTimeout + your server's expected response time`</span></code></pre>
]]></description><pubDate>Thu, 25 Apr 2019 21:29:00 GMT</pubDate><guid isPermaLink="false">https://shuheikagawa.com/blog/2019/04/25/keep-alive-timeout/</guid></item><item><title>2018 in Review</title><link>https://shuheikagawa.com/blog/2019/02/18/2018-in-review/</link><description><![CDATA[
<p>Looking back 2018, it flew like an arrow. It was so fast that it&#39;s already in February 2019!</p>
<p><img src="/images/tempelhof.jpg" alt="Sunset at Tempelhof in April"></p>
<h2 id="move">Move</h2>
<p>We had lived in an apartment on the border of Schöneberg and Wilmersdorf for 2 years, and decided to move out at the end of October without extending the contract. We spent two or three months for flat search, and a month and a half for moving, buying furniture and setting up the new apartment. After all, we like the new area and are looking forward to spend time on the balcony in the summer.</p>
<p>In the meanwhile, I got my left arm injured and it took a few months to recover.</p>
<h2 id="travel">Travel</h2>
<p>I visited two new countries and seven new cities. I wanted to visit a few more countries, but could not manage mainly because of the moving.</p>
<ul>
<li>Tokyo, Japan in Feburary</li>
<li>Spreewald, Germany in March</li>
<li>Amsterdam, Netherlands for React Amsterdam in April</li>
<li>Leipzig, Germany in May</li>
<li>Vienna, Austria for a wedding party in July</li>
<li>München, Germany for Oktoberfest in September</li>
<li>Köln and Düsseldorf, Germany in November</li>
</ul>
<h2 id="german-language">German Language</h2>
<p>After finishing an A2 course at office, I started a B1 course at <a href="https://www.speakeasysprachzeug.de/en">Speakeasy</a>. I felt that I should have taken A2 again... In the end, I was distracted by something else and stopped going to the course.</p>
<h2 id="work">Work</h2>
<p>It has been 2 years since I started working at Zalando. 2017 was about architecture migration from a monolith to microservices. 2018 was about optimization (and the next migration already started...).</p>
<p>In addition to front-end tasks, I focused more on non-feature stuff.</p>
<p>In the first half of the year, I focused on web (frontend) performance optimization. My team&#39;s work was featured in a blog post, <a href="https://jobs.zalando.com/tech/blog/loading-time-matters/">Loading Time Matters</a>, on the company blog.</p>
<p>In June, my team had a series of incidents on one of our applications, but we didn&#39;t know why. It opened a door of learning for me. I dug into Node.js internals and Linux network stack. I was lucky enough to find <a href="http://www.brendangregg.com/sysperfbook.html">Systems Performance by Brendan Gregg</a>, which is one of my all-time favorite technical books. As a by-product of the research/learning, I profiled Node.js servers on production and made some performance improvements. Wrote about it on <a href="/blog/2018/09/16/node-js-under-a-microscope/">Node.js under a Microscope: CPU FlameGraph and FlameScope</a>.</p>
<h2 id="side-projects">Side Projects</h2>
<p>I didn&#39;t worked on many side projects in 2018. Instead, I learned a lot of low-level stuff. Network, Linux, Node.js. I put some of what I learned into <a href="https://github.com/shuhei/knowledge">the knowledge repo</a> inspired by <a href="https://github.com/yoshuawuyts/knowledge">yoshuawuyts/knowledge</a>. Also, as a permanent solution for the issue at work, I wrote a library to keep Node.js app resilient against DNS timeouts, <a href="https://github.com/shuhei/pollen">pollen</a>. It&#39;s been working without issues for 1.5 months!</p>
<p>Some other unfinished pieces:</p>
<ul>
<li>Wrote some Haskell for <a href="https://github.com/shuhei/elm-compiler/pull/1">a GLSL parser</a> in the Elm compiler with <a href="https://github.com/w0rm">@w0rm</a>, but it&#39;s pending</li>
<li>Experimented Node.js profiling at <a href="https://github.com/shuhei/perf-playground">perf-playground</a></li>
<li>Played around with image formats at <a href="https://github.com/shuhei/incomplete-image-parser">incomplete-image-parser</a></li>
<li>Tried to write a Node.js profiler inspired by <a href="https://github.com/rbspy/rbspy">rbspy</a>, but gave up to figure out memory layout of V8 objects</li>
<li>Investigated an issue with <a href="https://github.com/facebook/react/issues/11538#issuecomment-390386520">React + Google Translate</a></li>
</ul>
<h2 id="2019">2019</h2>
<p>In 2018, I focused on tiny things such as shaving hundreds of milliseconds. In 2019, I would like to be more open. Try new things. Travel more.</p>

]]></description><pubDate>Tue, 19 Feb 2019 00:20:00 GMT</pubDate><guid isPermaLink="false">https://shuheikagawa.com/blog/2019/02/18/2018-in-review/</guid></item><item><title>Histogram for Time-Series Metrics on Node.js</title><link>https://shuheikagawa.com/blog/2018/12/29/histogram-for-time-series-metrics-on-node-js/</link><description><![CDATA[
<h2 id="the-metrics-library">The &quot;metrics&quot; library</h2>
<p>I have been using <a href="https://github.com/mikejihbe/metrics">metrics</a> library for application metrics of Node.js applications at work. It was already widely used in the company when I joined, and I kept using it without questioning much.</p>
<p>The <a href="https://github.com/mikejihbe/metrics">metrics</a> library was ported from <a href="https://github.com/dropwizard/metrics">Dropwizard metrics</a>, which is a widely-used metrics library for Java and also called as Coda Hale metrics, Yammer metrics, or Metrics Core. It supports various metrics types like Counter, Gauge, Histogram, Meter (a combination of Counter and Histogram), etc., and nice reporting abstraction.</p>
<p>Just before my last working day of 2018, I saw a weird chart with a p99.9 response time metric with only around 50 data points per minute. Outliers were staying for ~15 minutes (much longer than expected) and suddenly disappearing. I thought I was misusing the library. That&#39;s why I started reading the source code of <a href="https://github.com/mikejihbe/metrics">metrics</a> library, especially <code>Histogram</code>.</p>
<h2 id="eds-based-histogram">EDS-based Histogram</h2>
<p>The <code>metrics</code> library uses <a href="https://github.com/mikejihbe/metrics/blob/v0.1.20/stats/exponentially_decaying_sample.js">Exponentially Decaying Sample (EDS)</a> for <code>Histogram</code>. The name is intimidating, but the implementation is not so complicated.</p>
<p>It sets a priority to each value based on its timing and <strong>some randomness</strong>, and values of top-1028 priorities survive (by default). As a result, the chance of a value&#39;s survival decays as time goes by.</p>
<p>It seems to have a problem that the influence of an old value stays longer than expected, which was fixed in the Java implementation after the <code>metrics</code> library was ported to JavaScript. Maybe I can port the fix to the JavaScript implementation?</p>
<p>But, wait. Why do I need the decay at all? Most of my use cases of the histogram are to plot percentiles of response times. The data points are per minute. All I want for each data point is percentiles of all the response times <strong>measured in the last minute</strong>. I don&#39;t need response times from previous minutes because they are already plotted on the chart. Also, I don&#39;t want values in the last half of the minute to have more influence than values in the first half.
So, <strong>I don&#39;t need the decay effect at all</strong>.</p>
<p>In addition to that, EDS randomly ignores values. Yes, it <strong>samples</strong>. Random sampling is a problem because I&#39;m interested in a small number of outliers.</p>
<h2 id="hdr-histogram">HDR Histogram</h2>
<p>I tweeted about these issues, and <a href="https://twitter.com/cbirchall/status/1077526632951414784">my former colleague @cbirchall (Thanks!) suggested</a> to take a look at <a href="https://github.com/HdrHistogram/HdrHistogram">HdrHistogram</a>. I don&#39;t understand how it works (yet), but it claims to keep accuracy without sacrificing memory footprint and performance.</p>
<p><a href="https://medium.com/hotels-com-technology/your-latency-metrics-could-be-misleading-you-how-hdrhistogram-can-help-9d545b598374">Your Latency Metrics Could Be Misleading You — How HdrHistogram Can Help</a> by Will Tomlin on the Hotels.com Technology Blog illustrates shortcomings of the EDS-based histogram and advantages of the HDR histogram pretty well.</p>
<p>OK, I&#39;m sold.</p>
<h2 id="benchmark-on-nodejs">Benchmark on Node.js</h2>
<p>Then, how can I use HDR Histogram on Node.js? I found three implementations:</p>
<ul>
<li><a href="https://github.com/HdrHistogram/HdrHistogramJS">hdr-histogram-js</a>: JS implementation in the same GitHub org as the Java implementation</li>
<li><a href="https://github.com/mcollina/native-hdr-histogram">native-hdr-histogram</a>: A binding to a C implementation</li>
<li><a href="https://github.com/kiggundu/node-hdr-histogram">node-hdr-histogram</a>: A binding to the Java implementation</li>
</ul>
<p>Also EDS-based histogram implementations:</p>
<ul>
<li><a href="https://github.com/mikejihbe/metrics">metrics</a>: The library I&#39;m using at work</li>
<li><a href="https://github.com/yaorg/node-measured/tree/master/packages/measured-core">measured-core</a>: Actively maintained and widely used by Node.js developers (<a href="https://twitter.com/_vigneshh/status/1078287577394880512">Thanks @_vigneshh for letting me know!</a>)</li>
</ul>
<p>I compared them, excluding <code>node-hdr-histogram</code> because I think it&#39;s an overkill to run JVM only for metrics (and won&#39;t perform well anyway). The benchmark code is on <a href="https://gist.github.com/shuhei/3a747b26b62242ae795616b04c24024f">a gist</a>, and here is the result on Node.js 10.14.2.</p>
<p>Adding 10K values to a histogram:</p>
<ul>
<li>metrics: 173 ops/sec ±2.00% (80 runs sampled)</li>
<li>measured: 421 ops/sec ±1.19% (90 runs sampled)</li>
<li>hdr-histogram-js: 1,769 ops/sec ±1.84% (92 runs sampled)</li>
<li>native-hdr-histogram: 1,516 ops/sec ±0.82% (92 runs sampled)</li>
</ul>
<p>Extracting 12 different percentiles from a histogram:</p>
<ul>
<li>metrics: 1,721 ops/sec ±1.93% (92 runs sampled)</li>
<li>measured: 3,709 ops/sec ±0.78% (93 runs sampled)</li>
<li>measured (weighted percentiles): 2,383 ops/sec ±1.30% (90 runs sampled)</li>
<li>hdr-histogram-js: 3,509 ops/sec ±0.61% (93 runs sampled)</li>
<li>native-hdr-histogram: 2,760 ops/sec ±0.76% (93 runs sampled)</li>
</ul>
<p>According to the result, <code>hdr-histogram-js</code> is accurate and fast enough. Check out <a href="https://gist.github.com/shuhei/3a747b26b62242ae795616b04c24024f">the gist</a> for more details!</p>
<h2 id="reset-strategy">Reset Strategy</h2>
<p>While HDR Histogram can keep numbers more accurately than Exponentially Decaying Sample, it doesn&#39;t throw away old values by itself. We need a strategy to remove old values out of it. In a sense, EDS is a reset strategy. If we don&#39;t use it, we need another one.</p>
<p><a href="https://github.com/vladimir-bukhtoyarov/rolling-metrics/blob/e1bff04f05743b642585897182bb6807b1bdfce2/histograms.md#configuration-options-for-evicting-the-old-values-of-from-reservoir">Documentation of rolling-metrics library</a> lists up strategies and their trade-offs.</p>
<ul>
<li>Reset on snapshot</li>
<li>Reset periodically</li>
<li>Reset periodically by chunks (rolling time window)</li>
<li>Never reset</li>
</ul>
<p><em>Reset on snapshot</em> looks a bit hacky (we need to keep metrics collection only once in an interval) but should be easy to implement and practical. <em>Rolling time window</em> looks more rigorous, but a bit tedious to implement, especially about choosing the right parameters.</p>
<p>I made a quick survey of popular libraries and frameworks.</p>
<ul>
<li>Hysterix: <a href="https://github.com/Netflix/Hystrix/blob/v1.5.18/hystrix-core/src/main/java/com/netflix/hystrix/metric/consumer/RollingCommandLatencyDistributionStream.java">HdrHistogram + rolling time window</a></li>
<li>Finagle: <a href="https://github.com/twitter/finagle/blob/finagle-18.12.0/finagle-core/src/main/scala/com/twitter/finagle/util/WindowedPercentileHistogram.scala">HdrHistogram + rolling time window</a></li>
<li>Resilience4j: Uses Prometheus?</li>
<li>Prometheus: Supports <a href="https://prometheus.io/docs/practices/histograms/">Histogram and Summary</a> by its own implementation</li>
<li><a href="https://github.com/vladimir-bukhtoyarov/rolling-metrics">rolling-metrics</a>: Supports HdrHistogram and multiple strategies including rolling time window.</li>
<li><a href="https://github.com/erikvanoosten/metrics-scala">metrics-scala</a>: Supports HdrHistogram + only reset on snapshot strategy. Depends on <a href="https://bitbucket.org/marshallpierce/hdrhistogram-metrics-reservoir">hdrhistogram-metrics-reservoir</a>.</li>
</ul>
<p><em>Rolling time window</em> strategy seems to be most popular, but I couldn&#39;t find a consensus on default parameters (length of the time window, bucket size, etc.). For the next step, I&#39;ll probably start with <em>reset on snapshot</em> strategy and see if it works well.</p>
<p><strong>Update on Jan 11, 2019:</strong> I wrote <a href="https://github.com/shuhei/rolling-window">a package to use HDR histogram with rolling time window</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>HDR Histogram is more accurate than EDS-based Histogram for tracking response times in a time series. <a href="https://github.com/HdrHistogram/HdrHistogramJS">hdr-histogram-js</a> is accurate and performant. It seems to be the best option on Node.js. We need a way to remove old values from a histogram. <em>Reset on snapshot</em> is easy and practical, but <em>rolling time window</em> is more rigorous.</p>
<p>After the research on this topic, I got an impression that HDR Histogram is well-known in the Java/JVM community, but probably not so much in other communities. I made a benchmark on Node.js in this post, but it might be useful to review your metrics implementation on other programming languages or platforms as well.</p>

]]></description><pubDate>Sat, 29 Dec 2018 21:10:00 GMT</pubDate><guid isPermaLink="false">https://shuheikagawa.com/blog/2018/12/29/histogram-for-time-series-metrics-on-node-js/</guid></item><item><title>Node.js under a Microscope: CPU FlameGraph and FlameScope</title><link>https://shuheikagawa.com/blog/2018/09/16/node-js-under-a-microscope/</link><description><![CDATA[
<p>Last week, I had an opportunity to talk about profiling Node.js applications on production at an internal guild meeting at work. Here is a written version of it plus some additional information.</p>
<h2 id="background">Background</h2>
<p>I have been working on Node.js microservices, which fetch data from API servers and render HTML with React, at work. We monitor response times at load balancers, in application metrics and with distributed tracing with OpenTracing. One of the microservices had a weird gap between 99 percentile response times of itself and its dependencies. It was spending an extra 500 milliseconds—but I didn&#39;t know why.</p>
<p>My first suspect was the network. It is the place full of uncertainty. After learning and trying different commands and metrics, I took <code>tcpdump</code> and checked packets one by one with my eyes and a script. There were no significant delays that I had expected. So I had to stop blaming the network—or <em>someone else</em>.</p>
<h2 id="cpu-profiling-with-linux-perf-command">CPU Profiling with Linux <code>perf</code> Command</h2>
<p>Because the weird latency was happening in the application itself, I wanted to know what&#39;s going on in it. There are mainly two ways to achieve this: profiling and tracing. Profiling records some samples and tracing records everything. I wanted to do it <strong>on production</strong>, so profiling was naturally a good fit because of its smaller overhead.</p>
<p>For Node.js, there are mainly two different tools. One is <a href="https://github.com/v8/v8/wiki/V8-Profiler">V8 profiler</a>, and the other is <a href="https://perf.wiki.kernel.org/index.php/Main_Page">Linux perf</a>. V8 profiler uses the profiler provided by V8. It covers all JavaScript executions and V8 native functions. It works on non-Linux operating systems. If you use non-Linux machines, it might be pretty handy. On the other hand, Linux <code>perf</code> can profile almost anything including Linux kernel, libuv, and all processes on your OS with minimal overhead. However, as the name suggests, it works only on Linux. According to <a href="https://github.com/nodejs/diagnostics/issues/148">Node CPU Profiling Roadmap</a>, it seems that V8 profiler is the one officially supported by the V8 team, but Linux <code>perf</code> will keep working for a while. After all, I picked Linux <code>perf</code> because of low performance-overhead and small intervention to applications.</p>
<p>Linux <code>perf record</code> records stack traces into a binary file called <code>perf.data</code> by default. The binary file has only addresses and file names of functions. <code>perf script</code> converts the stack traces into a human-readable text file adding function names from program binaries and symbol map files.</p>
<pre><code class="hljs sh"><span class="hljs-comment"># Install dependencies for `perf` command</span>
sudo apt-get install linux-tools-common linux-tools-$(uname -r)
<span class="hljs-comment"># Test `perf` command</span>
sudo perf top

<span class="hljs-comment"># Record stack traces 99 times per second for 30 seconds</span>
sudo perf record -F 99 -p <span class="hljs-variable">${pid}</span> -g -- sleep 30s
<span class="hljs-comment"># Generate human readable stack traces</span>
sudo perf script &gt; stacks.<span class="hljs-variable">${pid}</span>.out</code></pre><p>Now we have human-readable stack traces, but it&#39;s still hard to browse thousands of stack traces and get insights from them. How can we efficiently analyze them?</p>
<h2 id="cpu-flame-graph">CPU Flame Graph</h2>
<p><a href="http://www.brendangregg.com/flamegraphs.html">CPU Flame Graph by Brendan Gregg</a> is a great way of visualizing stack traces. It aggregates stack traces into one chart. Frequently executed functions are shown wider and rarely executed functions are narrower in the chart.</p>
<p><img src="/images/flamegraph.png" alt="CPU Flame Graph">
<em>A CPU Flame Graph from <a href="https://github.com/shuhei/perf-playground">a sample application</a></em></p>
<p>I found some insights about the application on production with CPU Flame Graph:</p>
<ul>
<li>React server-side rendering is considered to be a very CPU-intensive task that blocks Node.js event loop. However, <code>JSON.parse()</code> was using 3x more CPU than React—it might be because we had already optimized React server-side rendering though.</li>
<li>Gzip decompression was using the almost same amount of CPU as React server-side rendering.</li>
</ul>
<p>There are a few tools like <a href="https://github.com/brendangregg/FlameGraph">FlameGraph</a> and <a href="https://github.com/davidmarkclements/0x">0x</a> to generate CPU Flame Graph from Linux <code>perf</code> stack traces. However, I eventually didn&#39;t need them because FlameScope, which I&#39;ll explain next, can generate CPU Flame Graph too.</p>
<h2 id="flamescope">FlameScope</h2>
<p><a href="https://github.com/Netflix/flamescope">FlameScope by Netflix</a> is another great tool for visualizing stack traces in a time-series. It shows a heatmap out of stack traces. Each cell represents a short amount of time, 20 ms if 50 cells per second, and its color represents how many times the process was on-CPU. It visualizes patterns of your application&#39;s activity.</p>
<p><img src="/images/flamescope-annotated.png" alt="FlameScope">
<em>Image from <a href="https://github.com/Netflix/flamescope">Netflix/flamescope</a></em></p>
<p>If you select a time range on the heatmap, FlameScope shows you a CPU Flame Graph of the range. It allows you to examine what happened when in details.</p>
<p>To use FlameScope, check out the repository and run the python server. Then put stack trace files from <code>perf script</code> into <code>examples</code> directory, and open <code>http://localhost:5000</code>.</p>
<p>I found a couple of exciting insights about the application on production using this tool.</p>
<h3 id="example-1-heavy-tasks-in-the-master-process">Example 1: Heavy Tasks in the Master Process</h3>
<p>The application used <a href="https://nodejs.org/api/cluster.html">the <code>cluster</code> module</a> to utilize multiple CPU cores. FlameScope showed that the master process was not busy for most of the time, but it occasionally kept using CPU for 1.5 seconds continuously! FlameScope showed that it was caused by metrics aggregation.</p>
<p>The master process was aggregating application metrics from worker processes, and it was responding to metrics collectors a few times in a minute. When the metrics collectors asked for data, the master process calculated percentiles of response times and prepared a JSON response. The percentile calculation was taking long time because the application had a lot of metrics buckets and the library that we used was using <code>JSON.stringify()</code> and <code>JSON.parse()</code> to deep-copy objects!</p>
<h3 id="example-2-frequent-garbage-collections">Example 2: Frequent Garbage Collections</h3>
<p>FlameScope showed that the worker processes were not overloaded for most of the time, but they had a few hundred milliseconds of CPU-busy time in about 10 seconds. It was caused by mark-sweep and mark-compact garbage collections.</p>
<p>The application had an in-memory fallback cache for API calls that was used only when API calls and retries fail. Even when API had problems, the cache hit rate was very low because of the number of permutations. In other words, it was not used almost at all. It cached large API responses for a while and threw them away after the cache expired. It looked innocent at first glance—but it was a problem for V8&#39;s <a href="http://www.memorymanagement.org/glossary/g.html#term-generational-garbage-collection">generational garbage collector</a>.</p>
<p>The API responses were always promoted to the old generation space causing frequent slow GCs. GC of the old generation is much slower than GC of the young generation. After removing the fallback cache, the application&#39;s 99 percentile response time improved by hundreds of milliseconds!</p>
<h2 id="nodejs-gotchas">Node.js Gotchas</h2>
<p><code>perf script</code> collects symbols for function addresses from program binaries. For Node.js, we need something special because functions are compiled just in time. As far as I know, there are two ways to record symbols:</p>
<ol>
<li>Run your Node.js process with <code>--perf-basic-prof-only-functions</code> option. It generates a log file at <code>/tmp/perf-${pid}.map</code>. The file keeps growing. The speed depends on your application, but it was a few megabytes per day for an application at work. Another problem is that functions in V8 keep moving and the addresses in <code>/tmp/perf-${pid}.map</code> get outdated. <a href="https://gist.github.com/shuhei/6c261342063bad387c70af384c6d8d5c">I wrote a script to fix the issue</a>.</li>
<li>Use <a href="https://github.com/mmarchini/node-linux-perf">mmarchini/node-linux-perf</a>. It generates the same <code>/tmp/perf-${pid}.map</code> as <code>--perf-basic-prof-only-functions</code> does, but on demand. Because it always freshly generates the file, it doesn&#39;t contain outdated symbols. It seems to be the way to go, but I haven&#39;t tried this on production yet.</li>
</ol>
<p>In addition to the above, there are a few more Node.js options that you can use to improve your stack traces—though I haven&#39;t tried them on production because the stack traces were already good enough for me:</p>
<ul>
<li><code>--no-turbo-inlining</code> turns off function inlining, which is an optimization done by V8. Because function inlining fuses multiple functions into one, it can make it harder to understand stack traces. Turning it off generates more named frames.</li>
<li><code>--interpreted-frames-native-stack</code> fixes <code>Builtin:InterpereterEntryTrampoline</code> in stack traces. It is available from Node.js 10.4.0. Check out &quot;Interpreted Frames&quot; in <a href="https://github.com/nodejs/diagnostics/issues/148#issuecomment-369348961">Updates from the Diagnostics Summit</a> for more details.</li>
</ul>
<h2 id="docker-gotchas">Docker Gotchas</h2>
<p>It gets a bit tricky when you are using containers to run your application. There are two ways to use Linux <code>perf</code> with Docker:</p>
<ol>
<li>Run <code>perf record</code> and <code>perf script</code> in the same Docker container as your application is running</li>
<li>Run <code>perf record</code> and <code>perf script</code> in the host OS</li>
</ol>
<p>I eventually chose the option 2. I tried the option 1 first but gave up because I was using Alpine Linux as the base image and it was hard to make Linux <code>perf</code> available on it.</p>
<p>To run <code>perf record</code> in the host OS, we need to figure out <code>pid</code> of the application process in the host.</p>
<pre><code class="hljs ">$ ps ax | grep -n &#39;node --perf&#39;
21574 pts/0    Sl+    2:53 node --perf-basic-prof-only-functions src/index.js
30481 pts/3    S+     0:00 grep --color=auto node --perf
# or
$ pgrep -f &#39;node --perf&#39;
21574

$ sudo perf record -F 99 -p 21574 -g -- sleep 30s</code></pre><p><code>perf script</code> collects symbols from binaries and symbol files to get human-readable function names. It needs to be able to read the binaries whose functions were recorded with <code>perf script</code> and <code>/tmp/${pid}.map</code> files that applications generate. However, <code>perf script</code> in the host OS cannot read them with the same file names as the container can. (It seems that this is not the case anymore with the latest Linux kernel because <a href="https://lkml.org/lkml/2017/7/19/790">its <code>perf</code> command knows containers</a>. But it was the case for me because I was not using the latest kernel.)</p>
<p>I learned how to overcome the issue from <a href="http://blog.alicegoldfuss.com/making-flamegraphs-with-containerized-java/">Making FlameGraphs with Containerized Java</a>. I just copied necessary files from the container to the host.</p>
<pre><code class="hljs sh"><span class="hljs-comment"># Horrible hack! Binaries to be used depend on your set up. `perf script` tells you what it wants if anything is missing.</span>
sudo docker cp mycontainer:/usr/bin/node /usr/bin/node
sudo docker cp mycontainer:/lib/ld-musl-x86_64.so.1 /lib/ld-musl-x86_64.so.1
sudo docker cp mycontainer:/usr/lib/libstdc++.so.6.0.22 /usr/lib/libstdc++.so.6.0.22</code></pre><p>To copy symbol map files, we need to find the <code>pid</code> in the container. We can do it by checking <code>/proc/${host_pid}/status</code>.</p>
<pre><code class="hljs ">$ cat /proc/21574/status | grep NSpid
NSpid:  21574   6
$ sudo docker cp mycontainer:/tmp/perf-6.map /tmp/perf-21574.map</code></pre><p>Now everything is ready! Then we can use <code>perf script</code> as usual.</p>
<pre><code class="hljs sh">sudo perf script &gt; stacks.<span class="hljs-variable">${pid}</span>.out</code></pre><p>I set up <a href="https://github.com/shuhei/perf-playground">a sample project</a> for profiling a Node.js application on Docker. It was nice to practice profiling a bit before doing it on production!</p>
<h2 id="conclusion">Conclusion</h2>
<p>Linux <code>perf</code> provides great observability to Node.js applications on production. Tools like CPU Flame Graph and FlameScope helped me to identify performance bottlenecks.</p>
<p>There are some gotchas to profile Node.js applications on Docker with Linux <code>perf</code>. It took some time for me to figure out how to do it because Node.js and Linux evolve day by day and I couldn&#39;t find many up-to-date resources online. I hope this post is helpful!</p>

]]></description><pubDate>Sun, 16 Sep 2018 08:56:00 GMT</pubDate><guid isPermaLink="false">https://shuheikagawa.com/blog/2018/09/16/node-js-under-a-microscope/</guid></item><item><title>2017 in Review</title><link>https://shuheikagawa.com/blog/2017/12/25/2017-in-review/</link><description><![CDATA[
<h2 id="berlin">Berlin</h2>
<p>I moved to Berlin from Tokyo at the end of September 2016. 2017 was my almost first year in Berlin.</p>
<p>I like the city so far. It is more relaxed than Tokyo and other big cities in Europe. Summer is especially nice. BBQ makes it even better. After my office moved to a building in front of Spree River, I enjoy my commute crossing Oberbaum Bridge and walking along the river.</p>
<h2 id="travels">Travels</h2>
<p>I traveled more than ever. The destinations were Germany (Dresden, Heidelberg, Frankfurt, Köln), Italy (Venice, Florence, Bologna), France (Paris), UK (London), Portugal (Lisbon) and Japan (Tokyo). I had fun in each of them, but if I have to choose one, I will name Lisbon. The city is full of what I miss in Berlin. Fresh and inexpensive seafood, views from hills, cute ceramic tiles, and beautiful weather. The sky was clear on every single day while I was there, and the highest temperature was 18 degrees in December!</p>
<h2 id="beer">Beer</h2>
<p>I am glad to have found <a href="https://untappd.com/fuerstwiacekbrew">Fuerst Wiacek</a>. Their <a href="https://untappd.com/b/fuerst-wiacek-german-movies/2155675">German Movies</a> is my No.1 beer so far. <a href="http://biererei-berlin.de/">Biererei</a> is a gem in Berlin, where I can buy fresh craft beers from Europe with growlers.</p>
<p>British ale was a discovery to me. I liked pubs in London a lot. I also attended <a href="https://www.brlohack.de/english/">the first craft beer hackathon in the world</a> and won 12 crates of craft beer...!</p>
<h2 id="shopping">Shopping</h2>
<p>I bought <a href="https://ergodox-ez.com/">an ergonomic keyboard</a> and <a href="https://billerbeck.info/en/products/82/neck-support-pillow-novum">a neck support pillow</a>. Both of them lifted up my quality of life. My body is getting older.</p>
<h2 id="language-learning">Language Learning</h2>
<p>I learned a bit of German Language. I finished A1 in May and started A2 after a pause of 5 months. While the learning process is prolonged, now German feels less cryptic to me.</p>
<h2 id="work">Work</h2>
<p>I was lucky to join an awesome team. We work together and hang out together. <a href="https://rework.withgoogle.com/blog/five-keys-to-a-successful-google-team/">A research at Google shows that psychological safety is a key to team effectiveness.</a> I feel it on my team.</p>
<p>On the technical side, my team joined a relatively large project and completed it on time. I worked mostly in architecture, performance optimization, type checking with Flow, SRE, etc. for apps with React and Node.js. I also helped my colleagues to start building an internal tool with Elm.</p>
<h2 id="side-projects">Side Projects</h2>
<p>I enjoyed working with Elm. I <a href="https://github.com/shuhei/elm-compare">wrote a mobile weather app</a>, flew to Paris for <a href="https://2017.elmeurope.org/">Elm Europe 2017</a>,  built <a href="https://github.com/shuhei/pixelm">a mobile-friendly pixel editor</a> and <a href="https://speakerdeck.com/shuhei/building-a-pixel-art-editor-with-elm">talked about it</a> at <a href="https://www.meetup.com/Elm-Berlin/events/242852794/">Elm Berlin Meetup</a>. I also helped <a href="https://github.com/w0rm/elm-glsl">an experiment of its compiler-side</a> in Haskell, although it is still pending.</p>
<p>I didn&#39;t do much with JavaScript for side projects but wrote <a href="https://github.com/shuhei/pelo">a tiny library for server-side rendering with tagged template literals</a> while hanging out with friends at a cafe. It&#39;s used in <a href="https://github.com/choojs">the choo ecosystem</a> now.</p>
<p>Aside from building things, I learned monad transformers, etc. from <a href="http://haskellbook.com/">Haskell Book</a> and machine learning with neural networks from <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization on Coursera</a>.</p>
<h2 id="then">.then()</h2>
<p>After all, I lived a year in a new country and enjoyed it. I have settled down, and now I feel prepared for new challenges next year. Let&#39;s see what is going to happen!</p>

]]></description><pubDate>Mon, 25 Dec 2017 21:54:00 GMT</pubDate><guid isPermaLink="false">https://shuheikagawa.com/blog/2017/12/25/2017-in-review/</guid></item></channel></rss>